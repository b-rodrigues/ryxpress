{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ryxpress \u2014 Reproducible Analytical Pipelines with Nix (Python)","text":"<p>If you\u2019re looking for <code>{rixpress}</code>, the R package version look here.</p> <p><code>ryxpress</code> is a Python reimplementation/port of the R package <code>{rixpress}</code>. It provides helpers and a small framework to build and work with reproducible, polyglot analytical pipelines that are built with Nix.</p> <p>The goal is to define a pipeline using the following R code:</p> <pre><code>library(rixpress)\nlibrary(igraph)\n\nlist(\n  rxp_py_file(\n    name = mtcars_pl,\n    path = 'data/mtcars.csv',\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n\n  rxp_py(\n    # reticulate doesn't support polars DFs yet, so need to convert\n    # first to pandas DF\n    name = mtcars_pl_am,\n    expr = \"mtcars_pl.filter(polars.col('am') == 1).to_pandas()\"\n  ),\n\n  rxp_py2r(\n    name = mtcars_am,\n    expr = mtcars_pl_am\n  ),\n\n  rxp_r(\n    name = mtcars_head,\n    expr = my_head(mtcars_am),\n    user_functions = \"functions.R\"\n  ),\n\n  rxp_r2py(\n    name = mtcars_head_py,\n    expr = mtcars_head\n  ),\n\n  rxp_py(\n    name = mtcars_tail_py,\n    expr = 'mtcars_head_py.tail()'\n  ),\n\n  rxp_py2r(\n    name = mtcars_tail,\n    expr = mtcars_tail_py\n  ),\n\n  rxp_r(\n    name = mtcars_mpg,\n    expr = dplyr::select(mtcars_tail, mpg)\n  ),\n\n  rxp_qmd(\n    name = page,\n    qmd_file = \"my_doc/page.qmd\",\n    additional_files = c(\"my_doc/content.qmd\", \"my_doc/images\")\n  )\n) |&gt;\n  rxp_populate(project_path = \".\", build = TRUE)\n</code></pre> <p><code>ryxpress</code> will execute an R session and call <code>{rixpress}</code> transparently to build this pipeline in a completely reproducible way. Exploring the build artifacts can then be done from an interactive Python session.</p> <p>If you previously used the R version (<code>{rixpress}</code>), <code>ryxpress</code> aims to provide a similar user experience for Python projects while integrating with the same Nix-first workflow.</p> <p>Video introduction (original R demo)</p>"},{"location":"#quick-overview","title":"Quick overview","text":"<ul> <li>Use Nix to describe reproducible runtime/build environments.</li> <li>Define pipeline derivations (build steps) in your project using R syntax, but   inspect and load artifacts using Python.</li> <li>Build pipelines with Nix and use ryxpress helpers to read, load or copy   outputs from the Nix store.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p><code>ryxpress</code> is on Pypi and can be installed using any of the usual package managers. That being said, since <code>ryxpress</code> requires both Nix and R to be available to function, we provide instructions for Nix only.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Nix installed on your machine. See the Nix project docs or Determinate Systems' installer.</li> </ul> <p>Because <code>ryxpress</code> is a wrapper around the R version, both R and <code>{rixpress}</code> need to be available, and since there\u2019s not much point in using <code>ryxpress</code> if you don\u2019t have Nix installed, the easiest way to install it is to build the environment as defined by this <code>default.nix</code>:</p> <pre><code>let\n pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-09-11.tar.gz\") {};\n\n rixpress = (pkgs.rPackages.buildRPackage {\n   name = \"rixpress\";\n   src = pkgs.fetchgit {\n     url = \"https://github.com/b-rodrigues/rixpress\";\n     rev = \"9a5dd6c31be9e6d413529924dd0816a510335881\";\n     sha256 = \"sha256-iQRo42RSnJ1C/ySCRyuaDt2MTP9G6g52wm+kkSHCir0=\";\n   };\n   propagatedBuildInputs = builtins.attrValues {\n     inherit (pkgs.rPackages)\n       igraph\n       jsonlite\n       processx;\n   };\n });\n\n  pyconf = builtins.attrValues {\n    inherit (pkgs.python313Packages)\n      pip\n      ipykernel\n      biocframe\n      pandas\n      rds2py\n      ryxpress;\n  };\n\n  system_packages = builtins.attrValues {\n    inherit (pkgs)\n      glibcLocales\n      nix\n      python313\n      R;\n  };\n\n  shell = pkgs.mkShell {\n    LOCALE_ARCHIVE = if pkgs.system == \"x86_64-linux\" then \"${pkgs.glibcLocales}/lib/locale/locale-archive\" else \"\";\n    LANG = \"en_US.UTF-8\";\n    LC_ALL = \"en_US.UTF-8\";\n    LC_TIME = \"en_US.UTF-8\";\n    LC_MONETARY = \"en_US.UTF-8\";\n    LC_PAPER = \"en_US.UTF-8\";\n    LC_MEASUREMENT = \"en_US.UTF-8\";\n    RETICULATE_PYTHON = \"${pkgs.python313}/bin/python\";\n\n    buildInputs = [ rixpress pyconf system_packages ];\n\n  };\nin\n  {\n    inherit pkgs shell;\n  }\n</code></pre> <p>You can change the date at the top to a more recent date to benefit from fresher packages. If you plan to use <code>uv</code> to manage Python packages, remove the <code>pyconf</code> block completely, and replace <code>python313</code> with <code>uv</code> in the <code>system_packages</code> block.</p>"},{"location":"#basic-usage-examples","title":"Basic usage examples","text":"<p>Create a pipeline as an R script:</p> <pre><code>library(rixpress)\n\nlist(\n  rxp_py_file(\n    name = dataset_np, # Keep name indicating NumPy array\n    path = \"data/pima-indians-diabetes.csv\",\n    read_function = \"lambda x: loadtxt(x, delimiter=',')\"\n  ),\n\n  rxp_py(\n    name = X,\n    expr = \"dataset_np[:,0:8]\"\n  ),\n\n  rxp_py(\n    name = Y,\n    expr = \"dataset_np[:,8]\"\n  ),\n\n  rxp_py(\n    name = splits,\n    expr = \"train_test_split(X, Y, test_size=0.33, random_state=7)\"\n  ),\n\n  # Extract X_train (index 0)\n  rxp_py(\n    name = X_train,\n    expr = \"splits[0]\"\n  ),\n\n  # Extract X_test (index 1)\n  rxp_py(\n    name = X_test,\n    expr = \"splits[1]\"\n  ),\n\n  # Extract y_train (index 2)\n  rxp_py(\n    name = y_train,\n    expr = \"splits[2]\"\n  ),\n\n  # Extract y_test (index 3)\n  rxp_py(\n    name = y_test,\n    expr = \"splits[3]\"\n  ),\n\n  rxp_py(\n    name = model,\n    expr = \"XGBClassifier(use_label_encoder=False, eval_metric='logloss').fit(X_train, y_train)\"\n  ),\n\n  rxp_py(\n    name = y_pred,\n    expr = \"model.predict(X_test)\"\n  ),\n\n  # Combine the y_test and y_pred vectors to export to csv\n  # This will be done used in an R environment by yardstick::conf_mat\n  rxp_py(\n    name = combined_df,\n    expr = \"DataFrame({'truth': y_test, 'estimate': y_pred})\"\n  ),\n\n  rxp_py(\n    name = combined_csv,\n    expr = \"combined_df\",\n    user_functions = \"functions.py\",\n    encoder = \"write_to_csv\"\n  ),\n\n  # yardstick::conf_mat needs factor variables\n  rxp_r(\n    combined_factor,\n    expr = mutate(\n      combined_csv,\n      across(.cols = everything(), .fns = factor)\n    ),\n    decoder = \"read.csv\"\n  ),\n\n  rxp_r(\n    name = confusion_matrix,\n    expr = conf_mat(\n      combined_factor,\n      truth,\n      estimate\n    )\n  ),\n\n  rxp_py(\n    name = accuracy,\n    expr = \"accuracy_score(y_test, y_pred)\"\n  )\n) |&gt;\n  rxp_populate(build = FALSE) # Need to set to FALSE because we\n# adjust imports first\n\nadjust_import(\n  \"import numpy\",\n  \"from numpy import array, loadtxt\"\n)\n\nadjust_import(\"import xgboost\", \"from xgboost import XGBClassifier\")\n\nadjust_import(\n  \"import sklearn\",\n  \"from sklearn.model_selection import train_test_split\"\n)\n\nadd_import(\"from sklearn.metrics import accuracy_score\", \"default.nix\")\nadd_import(\"from pandas import DataFrame\", \"default.nix\")\n</code></pre> <p>Start a Python session and:</p> <pre><code>from ryxpress import rxp_make\n\nrxp_make()\n</code></pre> <p>This will build the pipeline.</p>"},{"location":"#note-on-formats","title":"Note on formats:","text":"<ul> <li><code>rxp_read</code>/<code>rxp_load</code> will try <code>pickle.load</code> first</li> <li>If pickle fails <code>rxp_read</code>/<code>rxp_load</code> will attempt to use the optional rds2py   package (if present) to parse the file. This will load serialized R objects.</li> <li>If neither loader succeeds, the function returns the path(s).</li> </ul>"},{"location":"#inspect-builds-and-outputs","title":"Inspect builds and outputs","text":"<ul> <li><code>rxp_inspect</code> inspects the project build logs and helps resolve derivation outputs.</li> <li><code>rxp_copy</code> copies artifacts from <code>/nix/store</code> into your working directory for inspection.</li> <li><code>rxp_gc</code> helps manage cache/cleanup of local artifacts.</li> </ul>"},{"location":"#docs-and-api-reference-developer-docs","title":"Docs and API reference (developer docs)","text":"<p>This repository uses MkDocs + mkdocstrings to generate documentation and an autogenerated API reference from the package docstrings.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome. When contributing, please: - Provide small, focused, and runnable examples. - Prefer small datasets and short-running examples for <code>tests/docs</code>. - Document any system-level dependencies for examples in a <code>default.nix</code> so the pipeline can be reproduced.</p> <p>If you are unsure about a change, open an issue to discuss before submitting a PR. See <code>CONTRIBUTING.md</code> for guidelines (if present).</p>"},{"location":"#scope","title":"Scope","text":"<p>The Python port focuses on the same \u201cmicropipeline\u201d use case: single-machine pipelines for small-to-medium projects where Nix provides reproducible builds. It aims to mirror the user experience of the R package where practical, but it is not a drop-in replacement for all R-specific workflows. See the docs for current feature coverage and examples.</p>"},{"location":"#examples-demos","title":"Examples &amp; demos","text":"<p>See the examples and demos in the companion repository: https://github.com/b-rodrigues/rixpress_demos</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GNU General Public License v3.0 (GPL-3.0). See LICENSE for details.</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#start-a-project-and-build-the-pipeline","title":"Start a project and build the pipeline","text":"<p>Initialize rixpress project files in project_path. This will generate two R scripts: <code>gen-env.R</code>, which when executed using the rix R package will generate a <code>default.nix</code>, which defines the pipeline's execution environment, and <code>gen-pipeline.R</code>, which is where the pipeline is defined. These R scripts are the same as those generated by rixpress, the R version of this package.</p> <p>Parameters:</p> Name Type Description Default <code>project_path</code> <code>str</code> <p>path to the project directory (defaults to \".\")</p> <code>'.'</code> <code>skip_prompt</code> <code>bool</code> <p>if True, skip user confirmation prompts (defaults to False)</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if initialization completed (or was skipped due to non-interactive but files present),</p> <code>bool</code> <p>False if cancelled by the user.</p> Source code in <code>src/ryxpress/init_proj.py</code> <pre><code>def rxp_init(project_path: str = \".\", skip_prompt: bool = False) -&gt; bool:\n    \"\"\"\n    Initialize rixpress project files in project_path. This will generate\n    two R scripts: `gen-env.R`, which when executed using the rix R package\n    will generate a `default.nix`, which defines the pipeline's execution\n    environment, and `gen-pipeline.R`, which is where the pipeline is defined.\n    These R scripts are the same as those generated by rixpress, the R version\n    of this package.\n\n    Args:\n        project_path: path to the project directory (defaults to \".\")\n        skip_prompt: if True, skip user confirmation prompts (defaults to False)\n\n    Returns:\n        True if initialization completed (or was skipped due to non-interactive but files present),\n        False if cancelled by the user.\n    \"\"\"\n    # Initial confirmation before any action\n    if not _confirm(f\"Initialize project at '{project_path}'?\", skip_prompt=skip_prompt):\n        print(\"Operation cancelled by user. No files or directories were created.\")\n        return False\n\n    proj = Path(project_path)\n    # Ensure project_path exists, create it if it doesn't\n    if not proj.exists():\n        proj.mkdir(parents=True, exist_ok=True)\n\n    env_file = proj / \"gen-env.R\"\n    pipeline_file = proj / \"gen-pipeline.R\"\n\n    gen_env_lines = [\n        \"# This script defines the default environment the pipeline runs in.\",\n        \"# Add the required packages to execute the code necessary for each derivation.\",\n        \"# If you want to create visual representations of the pipeline, consider adding\",\n        \"# `{visNetwork}` and `{ggdag}` to the list of R packages.\",\n        \"library(rix)\",\n        \"\",\n        \"# Define execution environment\",\n        \"rix(\",\n        \"  date = NULL,\",\n        \"  r_pkgs = NULL,\",\n        \"  py_conf = NULL,\",\n        \"  git_pkgs = list(\",\n        \"    \\\"package_name\\\" = \\\"rixpress\\\",\",\n        \"    \\\"repo_url\\\" = \\\"https://github.com/b-rodrigues/rixpress\\\",\",\n        \"    \\\"commit\\\" = \\\"HEAD\\\",\",\n        \"  ),\",\n        \"  ide = \\\"none\\\",\",\n        \"  project_path = \\\".\\\"\",\n        \")\",\n    ]\n\n    gen_pipeline_lines = [\n        \"library(rixpress)\",\n        \"library(igraph)\",\n        \"\",\n        \"list(\",\n        \"  rxp_r_file(\",\n        \"    name = NULL,\",\n        \"    path = NULL,\",\n        \"    read_function = \\\"lambda x: polars.read_csv(x, separator='|')\\\"\",\n        \"  ),\",\n        \"  rxp_r(\",\n        \"    name = NULL,\",\n        \"    expr = NULL\",\n        \"  )\",\n        \") |&gt;\",\n        \"  rxp_populate(build = FALSE)\",\n    ]\n\n    # Write files (overwrite if present)\n    env_file.write_text(\"\\n\".join(gen_env_lines) + \"\\n\", encoding=\"utf-8\")\n    print(f\"File {env_file} has been written.\")\n    pipeline_file.write_text(\"\\n\".join(gen_pipeline_lines) + \"\\n\", encoding=\"utf-8\")\n    print(f\"File {pipeline_file} has been written.\")\n\n    # Skip Git initialization when on non-interactive sessions (CRAN/CI/test equivalent)\n    if not _is_interactive():\n        print(\n            \"Skipping Git initialization (non-interactive session, CRAN, CI, or test environment detected).\"\n        )\n        return True\n\n    # Ask whether to initialise git\n    if _confirm(\"Would you like to initialise a Git repository here?\", skip_prompt=skip_prompt):\n        git_bin = shutil.which(\"git\")\n        if git_bin is None:\n            print(\n                \"Git not found on PATH. Please install git and run 'git init' manually, \"\n                \"or initialise the repository using your preferred tool.\"\n            )\n        else:\n            try:\n                subprocess.run([git_bin, \"init\"], cwd=str(proj), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                print(\"Git repository initialised.\")\n            except subprocess.CalledProcessError as e:\n                print(\"Failed to initialise git repository. You can run 'git init' manually.\")\n    else:\n        print(\"Skipping Git initialization.\")\n\n    return True\n</code></pre> <p>Run the rixpress R pipeline (rxp_populate + rxp_make) by sourcing an R script.</p> <p>Parameters:</p> Name Type Description Default <code>script</code> <code>Union[str, Path]</code> <p>Path or name of the R script to run (defaults to \"gen-pipeline.R\"). If a relative path is given and doesn't exist in the working directory, this function will attempt to locate the script on PATH.</p> <code>'gen-pipeline.R'</code> <code>verbose</code> <code>int</code> <p>integer passed to rixpress::rxp_make(verbose = ...)</p> <code>0</code> <code>max_jobs</code> <code>int</code> <p>integer passed to rixpress::rxp_make(max_jobs = ...)</p> <code>1</code> <code>cores</code> <code>int</code> <p>integer passed to rixpress::rxp_make(cores = ...)</p> <code>1</code> <code>rscript_cmd</code> <code>str</code> <p>the Rscript binary to use (defaults to \"Rscript\")</p> <code>'Rscript'</code> <code>timeout</code> <code>Optional[int]</code> <p>optional timeout in seconds for the subprocess.run call</p> <code>None</code> <code>cwd</code> <code>Optional[Union[str, Path]]</code> <p>optional working directory to run Rscript in. If None, the directory containing the provided script will be used. This is important because pipeline.nix and related files are often imported with relative paths (e.g. ./default.nix), so Rscript needs to be run where those files are reachable.</p> <code>None</code> <p>Returns:</p> Type Description <code>RRunResult</code> <p>An RRunResult containing returncode, stdout, stderr.</p> Source code in <code>src/ryxpress/r_runner.py</code> <pre><code>def rxp_make(\n    script: Union[str, Path] = \"gen-pipeline.R\",\n    verbose: int = 0,\n    max_jobs: int = 1,\n    cores: int = 1,\n    rscript_cmd: str = \"Rscript\",\n    timeout: Optional[int] = None,\n    cwd: Optional[Union[str, Path]] = None,\n) -&gt; RRunResult:\n    \"\"\"\n    Run the rixpress R pipeline (rxp_populate + rxp_make) by sourcing an R script.\n\n    Args:\n        script: Path or name of the R script to run (defaults to \"gen-pipeline.R\").\n            If a relative path is given and doesn't exist in the working directory,\n            this function will attempt to locate the script on PATH.\n        verbose: integer passed to rixpress::rxp_make(verbose = ...)\n        max_jobs: integer passed to rixpress::rxp_make(max_jobs = ...)\n        cores: integer passed to rixpress::rxp_make(cores = ...)\n        rscript_cmd: the Rscript binary to use (defaults to \"Rscript\")\n        timeout: optional timeout in seconds for the subprocess.run call\n        cwd: optional working directory to run Rscript in. If None, the directory\n            containing the provided script will be used. This is important because\n            pipeline.nix and related files are often imported with relative paths\n            (e.g. ./default.nix), so Rscript needs to be run where those files are reachable.\n\n    Returns:\n        An RRunResult containing returncode, stdout, stderr.\n    \"\"\"\n    # Validate integers\n    for name, val in ((\"verbose\", verbose), (\"max_jobs\", max_jobs), (\"cores\", cores)):\n        if not isinstance(val, int):\n            raise TypeError(f\"{name} must be an int, got {type(val).__name__}\")\n        if val &lt; 0:\n            raise ValueError(f\"{name} must be &gt;= 0\")\n\n    # Resolve script path: prefer given path if it exists; otherwise try to find on PATH\n    script_path = Path(script)\n    if not script_path.is_file():\n        # If a bare name was provided, attempt to find it on PATH\n        found = shutil.which(str(script))\n        if found:\n            script_path = Path(found)\n        else:\n            raise FileNotFoundError(\n                f\"R script '{script}' not found in working directory and not on PATH\"\n            )\n    else:\n        script_path = script_path.resolve()\n\n    # Determine working directory for the R process:\n    if cwd is not None:\n        run_cwd = Path(cwd).resolve()\n        if not run_cwd.is_dir():\n            raise FileNotFoundError(f\"Requested cwd '{cwd}' does not exist or is not a directory\")\n    else:\n        # default to the script's parent directory so relative imports (./default.nix) work\n        run_cwd = script_path.parent\n\n    # Verify Rscript binary exists\n    if shutil.which(rscript_cmd) is None:\n        raise FileNotFoundError(\n            f\"Rscript binary '{rscript_cmd}' not found in PATH. Ensure R is installed or adjust rscript_cmd.\"\n        )\n\n    # Prepare wrapper R script that:\n    #  - loads rixpress,\n    #  - sources the user's script,\n    #  - if the sourced evaluation returns a list, calls rxp_populate on it,\n    #  - then calls rixpress::rxp_make(...) with the provided args.\n    wrapper = f\"\"\"\nsuppressPackageStartupMessages(library(rixpress))\n\nscript_path &lt;- \"{script_path.as_posix()}\"\n\nif (!file.exists(script_path)) {{\n  stop(\"Script not found: \", script_path)\n}}\n\nresult_value &lt;- NULL\n\nres &lt;- tryCatch({{\n  # Source &amp; evaluate the user's script and capture the returned value (if any)\n  result_value &lt;- eval(parse(script_path))\n  # If the script returned a list (a pipeline), run rxp_populate on it\n  if (!is.null(result_value) &amp;&amp; is.list(result_value)) {{\n    pipeline &lt;- result_value\n    pipeline &lt;- rixpress::rxp_populate(pipeline)\n  }}\n  # Finally, run rxp_make with the given integer parameters\n  rixpress::rxp_make(\n    verbose = {int(verbose)},\n    max_jobs = {int(max_jobs)},\n    cores = {int(cores)}\n  )\n}}, error = function(e) {{\n  # Print a clear error message and exit with non-zero status\n  message(\"rixpress-python-runner-error: \", conditionMessage(e))\n  quit(status = 1)\n}})\n\n# If we reach here, exit with success\nquit(status = 0)\n\"\"\"\n\n    # Create temporary file for wrapper\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".R\", delete=False) as tf:\n        tf.write(wrapper)\n        wrapper_path = Path(tf.name)\n\n    try:\n        # Run Rscript on the wrapper file using the desired working directory\n        proc = subprocess.run(\n            [rscript_cmd, str(wrapper_path)],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=timeout,\n            cwd=str(run_cwd),\n        )\n        return RRunResult(returncode=proc.returncode, stdout=proc.stdout, stderr=proc.stderr)\n    finally:\n        try:\n            wrapper_path.unlink()\n        except Exception:\n            pass\n</code></pre>"},{"location":"reference/#inspect-the-pipeline","title":"Inspect the pipeline","text":"<p>Inspect the build result of a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>project_path</code> <code>Union[str, Path]</code> <p>path to project root (defaults to \".\")</p> <code>'.'</code> <code>which_log</code> <code>Optional[str]</code> <p>optional regex to select a specific log file. If None, the most recent log is used.</p> <code>None</code> <code>pretty</code> <code>bool</code> <p>if True, pretty-prints the result (and returns nothing).</p> <code>False</code> <code>as_json</code> <code>bool</code> <p>if True, pretty prints using json.dumps(indent=2) instead of pprint.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[List[Dict[str, Any]]]</code> <p>A list of dict rows parsed from the selected JSON log file (unless pretty=True).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if no logs are found or _rixpress missing.</p> <code>ValueError</code> <p>if which_log is provided but no matching filename is found.</p> <code>RuntimeError</code> <p>if the chosen log cannot be read/parsed.</p> Source code in <code>src/ryxpress/inspect_logs.py</code> <pre><code>def rxp_inspect(\n    project_path: Union[str, Path] = \".\",\n    which_log: Optional[str] = None,\n    pretty: bool = False,\n    as_json: bool = False,\n) -&gt; Optional[List[Dict[str, Any]]]:\n    \"\"\"\n    Inspect the build result of a pipeline.\n\n    Args:\n        project_path: path to project root (defaults to \".\")\n        which_log: optional regex to select a specific log file. If None, the most recent log is used.\n        pretty: if True, pretty-prints the result (and returns nothing).\n        as_json: if True, pretty prints using json.dumps(indent=2) instead of pprint.\n\n    Returns:\n        A list of dict rows parsed from the selected JSON log file (unless pretty=True).\n\n    Raises:\n        FileNotFoundError: if no logs are found or _rixpress missing.\n        ValueError: if which_log is provided but no matching filename is found.\n        RuntimeError: if the chosen log cannot be read/parsed.\n    \"\"\"\n    proj = Path(project_path)\n    rixpress_dir = proj / \"_rixpress\"\n\n    logs = rxp_list_logs(proj)\n\n    chosen_path: Optional[Path] = None\n\n    if which_log is None:\n        chosen_path = rixpress_dir / logs[0][\"filename\"]\n    else:\n        import re, logging\n        logger = logging.getLogger(__name__)\n        pattern = re.compile(which_log)\n        for entry in logs:\n            if pattern.search(entry[\"filename\"]):\n                chosen_path = rixpress_dir / entry[\"filename\"]\n                logger.info(\"Using log file: %s\", entry[\"filename\"])\n                break\n        if chosen_path is None:\n            raise ValueError(f\"No build logs found matching the pattern: {which_log}\")\n\n    try:\n        with chosen_path.open(\"r\", encoding=\"utf-8\") as fh:\n            data = json.load(fh)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to read log file {chosen_path}: {e}\")\n\n    rows = _coerce_json_to_rows(data)\n\n    if pretty:\n        if as_json:\n            print(json.dumps(rows, indent=2, ensure_ascii=False))\n        else:\n            pprint(rows)\n        return  # This ensures REPL shows nothing after print, return value is None\n\n    return rows\n</code></pre> <p>List build logs in the project's _rixpress directory.</p> <p>Parameters:</p> Name Type Description Default <code>project_path</code> <code>Union[str, Path]</code> <p>path to project root (defaults to \".\")</p> <code>'.'</code> <code>pretty</code> <code>bool</code> <p>if True, pretty-prints the result (and returns nothing).</p> <code>False</code> <code>as_json</code> <code>bool</code> <p>if True, pretty prints using json.dumps(indent=2) instead of pprint.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[List[Dict[str, Union[str, float]]]]</code> <p>A list of dictionaries, each with keys:</p> <code>Optional[List[Dict[str, Union[str, float]]]]</code> <ul> <li>filename: basename of log file (str)</li> </ul> <code>Optional[List[Dict[str, Union[str, float]]]]</code> <ul> <li>modification_time: ISO date string YYYY-MM-DD (str)</li> </ul> <code>Optional[List[Dict[str, Union[str, float]]]]</code> <ul> <li>size_kb: file size in kilobytes rounded to 2 decimals (float)</li> </ul> <code>Optional[List[Dict[str, Union[str, float]]]]</code> <p>(unless pretty=True, in which case nothing is returned)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if the _rixpress directory does not exist or if no logs are found.</p> Source code in <code>src/ryxpress/inspect_logs.py</code> <pre><code>def rxp_list_logs(\n    project_path: Union[str, Path] = \".\",\n    pretty: bool = False,\n    as_json: bool = False,\n) -&gt; Optional[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    List build logs in the project's _rixpress directory.\n\n    Args:\n        project_path: path to project root (defaults to \".\")\n        pretty: if True, pretty-prints the result (and returns nothing).\n        as_json: if True, pretty prints using json.dumps(indent=2) instead of pprint.\n\n    Returns:\n        A list of dictionaries, each with keys:\n        - filename: basename of log file (str)\n        - modification_time: ISO date string YYYY-MM-DD (str)\n        - size_kb: file size in kilobytes rounded to 2 decimals (float)\n        (unless pretty=True, in which case nothing is returned)\n\n    Raises:\n        FileNotFoundError: if the _rixpress directory does not exist or if no logs are found.\n    \"\"\"\n    proj = Path(project_path)\n    rixpress_dir = proj / \"_rixpress\"\n\n    if not rixpress_dir.exists() or not rixpress_dir.is_dir():\n        raise FileNotFoundError(\"_rixpress directory not found. Did you initialise the project?\")\n\n    pattern = re.compile(r\"^build_log.*\\.json$\")\n    log_files = [p for p in rixpress_dir.iterdir() if p.is_file() and pattern.search(p.name)]\n\n    # Sort by modification time (most recent first)\n    log_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n\n    if not log_files:\n        raise FileNotFoundError(f\"No build logs found in {rixpress_dir}\")\n\n    logs: List[Dict[str, Union[str, float]]] = []\n    for p in log_files:\n        st = p.stat()\n        logs.append(\n            {\n                \"filename\": p.name,\n                \"modification_time\": _iso_date_from_epoch(st.st_mtime),\n                \"size_kb\": round(st.st_size / 1024.0, 2),\n            }\n        )\n\n    if pretty:\n        if as_json:\n            print(json.dumps(logs, indent=2, ensure_ascii=False))\n        else:\n            pprint(logs)\n        return\n\n    return logs\n</code></pre>"},{"location":"reference/#recover-artifacts","title":"Recover artifacts","text":"<p>Copy derivations from the Nix store to ./pipeline-output.</p> <p>Parameters:</p> Name Type Description Default <code>derivation_name</code> <code>Optional[str]</code> <p>name of the derivation to copy (string). If None, uses the special derivation name \"all-derivations\" (mirrors R).</p> <code>None</code> <code>dir_mode</code> <code>str</code> <p>octal permission string applied to copied directories (default \"0755\").</p> <code>'0755'</code> <code>file_mode</code> <code>str</code> <p>octal permission string applied to copied files (default \"0644\").</p> <code>'0644'</code> <code>project_path</code> <code>Union[str, Path]</code> <p>project root where _rixpress lives (defaults to \".\").</p> <code>'.'</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Prints a success message upon completion.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if _rixpress or logs are missing.</p> <code>ValueError</code> <p>on invalid modes or derivation not found.</p> <code>RuntimeError</code> <p>on copy failures.</p> Source code in <code>src/ryxpress/copy_artifacts.py</code> <pre><code>def rxp_copy(\n    derivation_name: Optional[str] = None,\n    dir_mode: str = \"0755\",\n    file_mode: str = \"0644\",\n    project_path: Union[str, Path] = \".\",\n) -&gt; None:\n    \"\"\"\n    Copy derivations from the Nix store to ./pipeline-output.\n\n    Args:\n        derivation_name: name of the derivation to copy (string). If None,\n            uses the special derivation name \"all-derivations\" (mirrors R).\n        dir_mode: octal permission string applied to copied directories (default \"0755\").\n        file_mode: octal permission string applied to copied files (default \"0644\").\n        project_path: project root where _rixpress lives (defaults to \".\").\n\n    Returns:\n        None. Prints a success message upon completion.\n\n    Raises:\n        FileNotFoundError: if _rixpress or logs are missing.\n        ValueError: on invalid modes or derivation not found.\n        RuntimeError: on copy failures.\n    \"\"\"\n    project = Path(project_path)\n    # Validate modes\n    if not _valid_mode(dir_mode):\n        raise ValueError('Invalid dir_mode: provide a character octal like \"0755\" or \"755\".')\n    if not _valid_mode(file_mode):\n        raise ValueError('Invalid file_mode: provide a character octal like \"0644\" or \"644\".')\n\n    # Ensure there is a build log\n    logs = rxp_list_logs(project)\n    # rxp_list_logs raises if none; if it returned, we have log entries\n\n    # Read latest build log content via rxp_inspect (most recent)\n    rows = rxp_inspect(project_path=project, which_log=None)\n    if not isinstance(rows, list) or not rows:\n        raise RuntimeError(\"Could not read build log details; rxp_inspect returned no rows.\")\n\n    # Build a mapping from derivation name -&gt; list of store paths\n    # We try to be tolerant: look for keys 'derivation' (R), then 'deriv', 'name'\n    deriv_key_candidates = (\"derivation\", \"deriv\", \"name\")\n    path_key_candidates = (\"path\", \"store_path\", \"path_store\", \"output_path\", \"output\")\n\n    deriv_to_paths: Dict[str, List[str]] = {}\n    for r in rows:\n        if not isinstance(r, dict):\n            continue\n        deriv_val = _extract_field(r, deriv_key_candidates)\n        path_val = _extract_field(r, path_key_candidates)\n        if deriv_val is None:\n            # skip rows without a derivation name\n            continue\n        derivs = _ensure_iterable_of_strings(deriv_val)\n        paths = _ensure_iterable_of_strings(path_val)\n        for d in derivs:\n            deriv_to_paths.setdefault(d, []).extend(paths)\n\n    # Deduplicate path lists\n    for k in list(deriv_to_paths.keys()):\n        seen = []\n        for p in deriv_to_paths[k]:\n            if p not in seen:\n                seen.append(p)\n        deriv_to_paths[k] = seen\n\n    # Choose derivation_name if not provided\n    if derivation_name is None:\n        derivation_name = \"all-derivations\"\n\n    if derivation_name not in deriv_to_paths:\n        # Provide hint of available derivations (up to 20)\n        available = list(deriv_to_paths.keys())[:20]\n        more = \", ...\" if len(deriv_to_paths) &gt; 20 else \"\"\n        raise ValueError(\n            f\"No derivation {derivation_name!r} found in the build log. Available: {', '.join(available)}{more}\"\n        )\n\n    # Collect paths for this derivation\n    deriv_paths = deriv_to_paths.get(derivation_name, [])\n    if not deriv_paths:\n        raise RuntimeError(f\"No store paths recorded for derivation {derivation_name!r} in the build log.\")\n\n    output_dir = _ensure_output_dir(Path.cwd())\n\n    # For each store path, copy its contents into output_dir\n    copy_failed = False\n    errors: List[str] = []\n    for store_path_str in deriv_paths:\n        store_path = Path(store_path_str)\n        if not store_path.exists():\n            # Skip non-existing path (warn)\n            logger.warning(\"Store path does not exist, skipping: %s\", store_path)\n            continue\n        try:\n            # If the derivation path is a directory, copy its children into output_dir\n            if store_path.is_dir():\n                # copy each child into output_dir, preserving names\n                for child in store_path.iterdir():\n                    dest = output_dir / child.name\n                    if child.is_dir():\n                        # Python 3.8+: dirs_exist_ok True will merge\n                        try:\n                            shutil.copytree(child, dest, dirs_exist_ok=True)\n                        except TypeError:\n                            # older Python: fallback to manual merge\n                            if dest.exists():\n                                # copy contents into existing dest\n                                for sub in child.rglob(\"*\"):\n                                    rel = sub.relative_to(child)\n                                    target = dest / rel\n                                    if sub.is_dir():\n                                        target.mkdir(parents=True, exist_ok=True)\n                                    else:\n                                        target.parent.mkdir(parents=True, exist_ok=True)\n                                        shutil.copy2(sub, target)\n                            else:\n                                shutil.copytree(child, dest)\n                    else:\n                        # file: copy, possibly overwrite\n                        shutil.copy2(child, dest)\n            else:\n                # store_path is a file: copy into output_dir\n                dest_file = output_dir / store_path.name\n                shutil.copy2(store_path, dest_file)\n        except Exception as e:\n            copy_failed = True\n            errors.append(f\"{store_path}: {e}\")\n            logger.debug(\"Copy error for %s: %s\", store_path, e)\n\n    # Apply permissions\n    try:\n        _apply_permissions(output_dir, dir_mode=dir_mode, file_mode=file_mode)\n    except Exception:\n        # Best-effort: ignore permission application errors\n        logger.debug(\"Failed to apply permissions to %s\", output_dir)\n\n    if copy_failed:\n        raise RuntimeError(f\"Copy unsuccessful: errors occurred:\\n\" + \"\\n\".join(errors))\n\n    # Success message\n    print(f\"Copy successful, check out {output_dir}\")\n    return None\n</code></pre> <p>Read the output of a derivation.</p> <p>Parameters:</p> Name Type Description Default <code>derivation_name</code> <code>str</code> <p>name of the derivation to read.</p> required <code>which_log</code> <code>Optional[str]</code> <p>optional regex to select a specific log file. If None, the most recent log is used.</p> <code>None</code> <code>project_path</code> <code>Union[str, Path]</code> <p>path to project root (defaults to \".\").</p> <code>'.'</code> <p>Returns:</p> Type Description <code>Union[object, str, List[str]]</code> <p>The loaded object if successfully unpickled or parsed via rds2py.</p> <code>Union[object, str, List[str]]</code> <p>Otherwise, returns the path string (or list of paths if multiple outputs).</p> Note <p>All failures are silent; no exceptions/warnings are raised for \"can't load\" cases.</p> Source code in <code>src/ryxpress/read_load.py</code> <pre><code>def rxp_read(\n    derivation_name: str,\n    which_log: Optional[str] = None,\n    project_path: Union[str, Path] = \".\",\n) -&gt; Union[object, str, List[str]]:\n    \"\"\"\n    Read the output of a derivation.\n\n    Args:\n        derivation_name: name of the derivation to read.\n        which_log: optional regex to select a specific log file. If None, the most recent log is used.\n        project_path: path to project root (defaults to \".\").\n\n    Returns:\n        The loaded object if successfully unpickled or parsed via rds2py.\n        Otherwise, returns the path string (or list of paths if multiple outputs).\n\n    Note:\n        All failures are silent; no exceptions/warnings are raised for \"can't load\" cases.\n    \"\"\"\n    resolved = rxp_read_load_setup(derivation_name, which_log=which_log, project_path=project_path)\n\n    # If multiple outputs (list), return them directly\n    if isinstance(resolved, list):\n        return resolved\n\n    # Single path (string) or fallback value (derivation_name)\n    path = str(resolved)\n\n    # If path points to a directory, return it\n    if os.path.isdir(path):\n        return path\n\n    # Try to unpickle first (regardless of extension)\n    try:\n        with open(path, \"rb\") as fh:\n            obj = pickle.load(fh)\n        return obj\n    except Exception:\n        # Silent failure \u2014 try the next loader\n        logger.debug(\"pickle load failed for %s; will try rds2py if available\", path, exc_info=True)\n\n    # Try rds2py as a fallback (regardless of extension)\n    rds_obj = _load_rds_with_rds2py(path)\n    if rds_obj is not None:\n        return rds_obj\n\n    # Nothing worked; return the path string (no errors/warnings)\n    return path\n</code></pre> <p>Load the output of a derivation into the caller's globals.</p> <p>Parameters:</p> Name Type Description Default <code>derivation_name</code> <code>str</code> <p>name of the derivation to load. Also used as the variable name in globals.</p> required <code>which_log</code> <code>Optional[str]</code> <p>optional regex to select a specific log file. If None, the most recent log is used.</p> <code>None</code> <code>project_path</code> <code>Union[str, Path]</code> <p>path to project root (defaults to \".\").</p> <code>'.'</code> <p>Returns:</p> Type Description <code>Union[object, str, List[str]]</code> <p>The loaded object if successfully unpickled or parsed.</p> <code>Union[object, str, List[str]]</code> <p>Otherwise, returns the path string (or list of paths if multiple outputs).</p> Note <p>The loaded object is assigned to the caller's globals under <code>derivation_name</code>. All failures are silent.</p> Source code in <code>src/ryxpress/read_load.py</code> <pre><code>def rxp_load(\n    derivation_name: str,\n    which_log: Optional[str] = None,\n    project_path: Union[str, Path] = \".\",\n) -&gt; Union[object, str, List[str]]:\n    \"\"\"\n    Load the output of a derivation into the caller's globals.\n\n    Args:\n        derivation_name: name of the derivation to load. Also used as the variable name in globals.\n        which_log: optional regex to select a specific log file. If None, the most recent log is used.\n        project_path: path to project root (defaults to \".\").\n\n    Returns:\n        The loaded object if successfully unpickled or parsed.\n        Otherwise, returns the path string (or list of paths if multiple outputs).\n\n    Note:\n        The loaded object is assigned to the caller's globals under `derivation_name`.\n        All failures are silent.\n    \"\"\"\n    resolved = rxp_read_load_setup(derivation_name, which_log=which_log, project_path=project_path)\n\n    # If multiple outputs, return them\n    if isinstance(resolved, list):\n        return resolved\n\n    path = str(resolved)\n\n    if os.path.isdir(path):\n        return path\n\n    # Try to unpickle first\n    try:\n        with open(path, \"rb\") as fh:\n            obj = pickle.load(fh)\n    except Exception:\n        obj = None\n        logger.debug(\"pickle load failed for %s; will try rds2py if available\", path, exc_info=True)\n\n    # If pickle failed, try rds2py\n    if obj is None:\n        obj = _load_rds_with_rds2py(path)\n\n    if obj is None:\n        # Nothing we can load silently; return the path\n        return path\n\n    # Assign into caller's globals (best-effort); silence any assignment errors\n    try:\n        caller_frame = inspect.currentframe().f_back\n        if caller_frame is not None:\n            caller_globals = caller_frame.f_globals\n            # Use derivation_name as the variable name; keep last path component if it's a path\n            try:\n                var_name = derivation_name\n                # If derivation_name looks like a path, use the basename without extension\n                if derivation_name.startswith(\"/nix/store/\") or os.path.sep in derivation_name:\n                    var_name = os.path.splitext(os.path.basename(str(path)))[0]\n                # ensure valid identifier fallback\n                if not var_name.isidentifier():\n                    var_name = \"_\".join(re.findall(r\"\\w+\", var_name)) or \"loaded_artifact\"\n            except Exception:\n                var_name = \"loaded_artifact\"\n            caller_globals[var_name] = obj\n    except Exception:\n        logger.debug(\"Failed to assign loaded object into caller globals\", exc_info=True)\n\n    return obj\n</code></pre>"},{"location":"reference/#visually-exploring-the-pipeline","title":"Visually exploring the pipeline","text":"<p>Build an igraph object from nodes_and_edges and write a DOT file for CI.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_and_edges</code> <code>Optional[Dict[str, List[Dict]]]</code> <p>dict with keys 'nodes' and 'edges' as returned by get_nodes_edges(). If None, get_nodes_edges() is called.</p> <code>None</code> <code>output_file</code> <code>Union[str, Path]</code> <p>path to write DOT file. Parent directories are created as needed.</p> <code>'_rixpress/dag.dot'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>if python-igraph is not installed.</p> Source code in <code>src/ryxpress/plotting.py</code> <pre><code>def rxp_dag_for_ci(nodes_and_edges: Optional[Dict[str, List[Dict]]] = None,\n                   output_file: Union[str, Path] = \"_rixpress/dag.dot\") -&gt; None:\n    \"\"\"\n    Build an igraph object from nodes_and_edges and write a DOT file for CI.\n\n    Args:\n        nodes_and_edges: dict with keys 'nodes' and 'edges' as returned by\n            get_nodes_edges(). If None, get_nodes_edges() is called.\n        output_file: path to write DOT file. Parent directories are created as needed.\n\n    Raises:\n        ImportError: if python-igraph is not installed.\n    \"\"\"\n    # Lazy import igraph and raise helpful error if not available\n    try:\n        import igraph  # python-igraph\n    except Exception as e:  # ImportError or other import-time errors\n        raise ImportError(\n            \"The python 'igraph' package is required for rxp_dag_for_ci. \"\n            \"Install it with e.g. 'pip install python-igraph' and try again.\"\n        ) from e\n\n    if nodes_and_edges is None:\n        nodes_and_edges = get_nodes_edges()\n\n    edges = nodes_and_edges.get(\"edges\", [])\n    # Build a list of tuples (from, to) for igraph\n    edge_tuples = [(e[\"from\"], e[\"to\"]) for e in edges]\n\n    # Ensure output directory exists\n    out_path = Path(output_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Create the graph from edge tuples. TupleList will create vertices named by\n    # the unique labels encountered in the tuples.\n    # If there are no edges but there are nodes, create an empty graph and add vertices.\n    if edge_tuples:\n        g = igraph.Graph.TupleList(edge_tuples, directed=True, vertex_name_attr=\"name\")\n    else:\n        # no edges \u2014 create graph and add vertices from nodes list\n        nodes = nodes_and_edges.get(\"nodes\", [])\n        vertex_names = [n[\"id\"] for n in nodes]\n        g = igraph.Graph(directed=True)\n        if vertex_names:\n            g.add_vertices(vertex_names)\n            # set the 'name' attribute automatically when vertices are named\n\n    # Set vertex 'label' attribute from vertex name\n    # g.vs['name'] should exist; copy to 'label'\n    try:\n        names = g.vs[\"name\"]\n        g.vs[\"label\"] = names\n        # Attempt to remove the 'name' attribute to mirror R behavior.\n        # python-igraph allows deleting vertex attributes via 'del g.vs[\"attr\"]'.\n        try:\n            del g.vs[\"name\"]\n        except Exception:\n            # If deletion is not supported in some igraph versions, leave it;\n            # having both 'name' and 'label' is harmless for DOT output.\n            logger.debug(\"Could not delete 'name' vertex attribute; leaving it in place.\")\n    except Exception:\n        # If the graph has no vertices or attribute access fails, continue.\n        pass\n\n    # Write graph to DOT format\n    # Use Graph.write with format=\"dot\"\n    try:\n        g.write(str(out_path), format=\"dot\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to write DOT file to {out_path}: {e}\") from e\n</code></pre> <p>Render a DOT graph file as an ASCII diagram using phart, showing node labels.</p> <p>This function reads a DOT file, parses it with pydot and networkx, and renders it in ASCII using phart. Node labels from the DOT file are used instead of numeric node IDs.</p> Dependencies <ul> <li>phart</li> <li>pydot</li> <li>networkx</li> </ul> <p>Make sure to add these dependencies to the execution environment to use this function.</p> <p>Parameters:</p> Name Type Description Default <code>dot_path</code> <code>str</code> <p>Path to the DOT file to render.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified DOT file does not exist.</p> <code>ValueError</code> <p>If the DOT file is empty or cannot be parsed into a graph.</p> Source code in <code>src/ryxpress/plotting.py</code> <pre><code>def rxp_phart(dot_path: str) -&gt; None:\n    \"\"\"\n    Render a DOT graph file as an ASCII diagram using phart, showing node labels.\n\n    This function reads a DOT file, parses it with pydot and networkx, and\n    renders it in ASCII using phart. Node labels from the DOT file are used\n    instead of numeric node IDs.\n\n    Dependencies:\n        - phart\n        - pydot\n        - networkx\n\n    Make sure to add these dependencies to the execution environment to use this function.\n\n    Args:\n        dot_path: Path to the DOT file to render.\n\n    Raises:\n        FileNotFoundError: If the specified DOT file does not exist.\n        ValueError: If the DOT file is empty or cannot be parsed into a graph.\n    \"\"\"\n\n    # Dependency checks\n    missing = []\n    try:\n        import phart\n        from phart import ASCIIRenderer\n    except ImportError:\n        missing.append(\"phart\")\n    try:\n        import pydot\n    except ImportError:\n        missing.append(\"pydot\")\n    try:\n        import networkx as nx\n    except ImportError:\n        missing.append(\"networkx\")\n\n    if missing:\n        print(\n            f\"The following dependencies are required but not installed: {', '.join(missing)}\"\n        )\n        print(f\"Please add them to the execution environment.\")\n        return\n\n    # Check file exists\n    import os\n    if not os.path.exists(dot_path):\n        raise FileNotFoundError(f\"DOT file not found: {dot_path}\")\n\n    # Load DOT file\n    with open(dot_path) as f:\n        dot_data = f.read()\n\n    if not dot_data.strip():\n        raise ValueError(\"DOT file is empty.\")\n\n    # Parse DOT into networkx graph\n    graphs = pydot.graph_from_dot_data(dot_data)\n    if not graphs:\n        raise ValueError(\"No valid graphs found in DOT file.\")\n\n    G = nx.nx_pydot.from_pydot(graphs[0])\n\n    # Map node keys to labels for display\n    mapping = {node: data.get(\"label\", str(node)) for node, data in G.nodes(data=True)}\n    H = nx.relabel_nodes(G, mapping)\n\n    # Render ASCII\n    renderer = ASCIIRenderer(H)\n    print(renderer.render())\n</code></pre> <p>Trace lineage of derivations.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Name of the derivation to trace. If None, traces the whole pipeline.</p> <code>None</code> <code>dag_file</code> <code>Union[str, Path]</code> <p>Path to the dag.json file (defaults to \"_rixpress/dag.json\").</p> <code>Path('_rixpress') / 'dag.json'</code> <code>transitive</code> <code>bool</code> <p>If True, include transitive dependencies marked with '*'.</p> <code>True</code> <code>include_self</code> <code>bool</code> <p>If True, include the node itself in dependency lists.</p> <code>False</code> <code>color</code> <code>bool</code> <p>If True and derivations have pipeline_color, names are coloured in output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, List[str]]]</code> <p>A dict mapping each inspected derivation name to a dict with keys:</p> <code>Dict[str, Dict[str, List[str]]]</code> <ul> <li>'dependencies' : list of dependency names (ancestors), with transitive-only names marked with '*'</li> </ul> <code>Dict[str, Dict[str, List[str]]]</code> <ul> <li>'reverse_dependencies' : list of reverse dependents (children), with transitive-only names marked with '*'</li> </ul> Side-effect <p>Prints a tree representation to stdout (either the whole pipeline or the single-node lineage). When color=True and terminal supports it, derivation names are coloured by their pipeline_color.</p> Source code in <code>src/ryxpress/tracing.py</code> <pre><code>def rxp_trace(\n    name: Optional[str] = None,\n    dag_file: Union[str, Path] = Path(\"_rixpress\") / \"dag.json\",\n    transitive: bool = True,\n    include_self: bool = False,\n    color: bool = True,\n) -&gt; Dict[str, Dict[str, List[str]]]:\n    \"\"\"\n    Trace lineage of derivations.\n\n    Args:\n        name: Name of the derivation to trace. If None, traces the whole pipeline.\n        dag_file: Path to the dag.json file (defaults to \"_rixpress/dag.json\").\n        transitive: If True, include transitive dependencies marked with '*'.\n        include_self: If True, include the node itself in dependency lists.\n        color: If True and derivations have pipeline_color, names are coloured in output.\n\n    Returns:\n        A dict mapping each inspected derivation name to a dict with keys:\n        - 'dependencies' : list of dependency names (ancestors), with transitive-only names marked with '*'\n        - 'reverse_dependencies' : list of reverse dependents (children), with transitive-only names marked with '*'\n\n    Side-effect:\n        Prints a tree representation to stdout (either the whole pipeline or\n        the single-node lineage). When color=True and terminal supports it,\n        derivation names are coloured by their pipeline_color.\n    \"\"\"\n    derivs, color_map = _load_dag(dag_file)\n\n    # Check if we should use colour\n    use_color = color and _supports_color()\n\n    # Helper to get coloured name\n    def maybe_color(node_name: str, suffix: str = \"\") -&gt; str:\n        \"\"\"Return node name with optional colour and suffix.\"\"\"\n        base_name = node_name.rstrip(\"*\")\n        star = \"*\" if node_name.endswith(\"*\") else \"\"\n        display_name = base_name + star + suffix\n\n        if use_color:\n            hex_color = color_map.get(base_name)\n            if hex_color:\n                ansi = _hex_to_ansi(hex_color)\n                return _colorize(display_name, ansi)\n        return display_name\n\n    all_names: List[str] = []\n    for d in derivs:\n        nm = _extract_name(d)\n        if nm is None:\n            raise ValueError(\"Found derivations with missing or unparsable names in dag.json.\")\n        all_names.append(nm)\n\n    if name is not None and name not in all_names:\n        # mirror R's head(...) behaviour for listing available names\n        snippet = \", \".join(all_names[:20])\n        more = \", ...\" if len(all_names) &gt; 20 else \"\"\n        raise ValueError(f\"Derivation '{name}' not found in dag.json (available: {snippet}{more}).\")\n\n    depends_map = _make_depends_map(derivs, all_names)\n    reverse_map = _build_reverse_map(depends_map, all_names)\n\n    # helper to print single lineage (deps and reverse deps)\n    def print_single(target: str) -&gt; None:\n        print(f\"==== Lineage for: {maybe_color(target)} ====\")\n        # Dependencies (ancestors)\n        print(\"Dependencies (ancestors):\")\n        visited: List[str] = []\n\n        def rec_dep(node: str, depth: int) -&gt; None:\n            parents = depends_map.get(node) or []\n            if not parents:\n                if depth == 0:\n                    print(\"  - &lt;none&gt;\")\n                return\n            for p in parents:\n                label = f\"{p}*\" if (transitive and depth &gt;= 1) else p\n                print((\"  \" * (depth + 1)) + \"- \" + maybe_color(label))\n                if p not in visited:\n                    visited.append(p)\n                    rec_dep(p, depth + 1)\n\n        rec_dep(target, 0)\n\n        print(\"\\nReverse dependencies (children):\")\n        visited = []\n\n        def rec_rev(node: str, depth: int) -&gt; None:\n            kids = reverse_map.get(node) or []\n            if not kids:\n                if depth == 0:\n                    print(\"  - &lt;none&gt;\")\n                return\n            for k in kids:\n                label = f\"{k}*\" if (transitive and depth &gt;= 1) else k\n                print((\"  \" * (depth + 1)) + \"- \" + maybe_color(label))\n                if k not in visited:\n                    visited.append(k)\n                    rec_rev(k, depth + 1)\n\n        rec_rev(target, 0)\n\n        if transitive:\n            print(\"\\nNote: '*' marks transitive dependencies (depth &gt;= 2).\\n\")\n\n    # helper to print forest starting from given roots, using depends_map (outputs -&gt; inputs)\n    def print_forest_once(roots: List[str], graph: Dict[str, List[str]], transitive_flag: bool) -&gt; None:\n        visited_nodes: List[str] = []\n\n        def rec(node: str, depth: int) -&gt; None:\n            label = f\"{node}*\" if (transitive_flag and depth &gt;= 2) else node\n            print((\"  \" * depth) + \"- \" + maybe_color(label))\n            if node in visited_nodes:\n                return\n            visited_nodes.append(node)\n            kids = graph.get(node) or []\n            if not kids:\n                return\n            for k in kids:\n                rec(k, depth + 1)\n\n        for r in roots:\n            rec(r, 0)\n\n    # sinks: nodes with no children in reverse_map\n    def sinks() -&gt; List[str]:\n        no_children = [n for n, kids in reverse_map.items() if not kids]\n        if no_children:\n            return no_children\n        outdeg_vals = {n: len(kids) for n, kids in reverse_map.items()}\n        if outdeg_vals:\n            min_outdeg = min(outdeg_vals.values())\n            return [n for n, v in outdeg_vals.items() if v == min_outdeg]\n        return []\n\n    # Build results mapping\n    results: Dict[str, Dict[str, List[str]]] = {}\n    for nm in all_names:\n        deps = _marked_vec(nm, depends_map, transitive)\n        rdeps = _marked_vec(nm, reverse_map, transitive)\n        if include_self:\n            deps = _unique_preserve_order([nm] + deps)\n            rdeps = _unique_preserve_order([nm] + rdeps)\n        results[nm] = {\"dependencies\": deps, \"reverse_dependencies\": rdeps}\n\n    if name is None:\n        print(\"==== Pipeline dependency tree (outputs \\u2192 inputs) ====\")\n        for root in sinks():\n            print_forest_once([root], depends_map, transitive)\n        if transitive:\n            print(\"\\nNote: '*' marks transitive dependencies (depth &gt;= 2).\\n\")\n        return results\n    else:\n        print_single(name)\n        # return only the single-name mapping to match the R invisible(results[name]) behaviour\n        return {name: results[name]}\n</code></pre>"},{"location":"reference/#utilities","title":"Utilities","text":"<p>Garbage collect Nix store paths and build logs produced by rixpress.</p> <p>Parameters:</p> Name Type Description Default <code>keep_since</code> <code>Optional[Union[str, date]]</code> <p>None for full GC, or a date/ISO date string (YYYY-MM-DD) to keep logs newer-or-equal to that date.</p> <code>None</code> <code>project_path</code> <code>Union[str, Path]</code> <p>project root containing _rixpress</p> <code>'.'</code> <code>dry_run</code> <code>bool</code> <p>if True, show what would be deleted without deleting</p> <code>True</code> <code>timeout_sec</code> <code>int</code> <p>timeout for invoked nix-store commands and for lock staleness checks</p> <code>300</code> <code>verbose</code> <code>bool</code> <p>if True, print extra diagnostic output</p> <code>False</code> <code>ask</code> <code>bool</code> <p>if True, prompt for confirmation before destructive operations (default True)</p> <code>True</code> <code>pretty</code> <code>bool</code> <p>if True, pretty-prints the result (and returns nothing).</p> <code>False</code> <code>as_json</code> <code>bool</code> <p>if True, pretty prints using json.dumps(indent=2) instead of pprint.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, object]</code> <p>A summary dict with canonical keys:</p> <code>Dict[str, object]</code> <p>kept, deleted, protected, deleted_count, failed_count, referenced_count,</p> <code>Dict[str, object]</code> <p>log_files_deleted, log_files_failed, dry_run_details</p> Source code in <code>src/ryxpress/garbage.py</code> <pre><code>def rxp_gc(\n    keep_since: Optional[Union[str, date]] = None,\n    project_path: Union[str, Path] = \".\",\n    dry_run: bool = True,\n    timeout_sec: int = 300,\n    verbose: bool = False,\n    ask: bool = True,\n    pretty: bool = False,\n    as_json: bool = False,\n) -&gt; Dict[str, object]:\n    \"\"\"\n    Garbage collect Nix store paths and build logs produced by rixpress.\n\n    Args:\n        keep_since: None for full GC, or a date/ISO date string (YYYY-MM-DD) to keep logs newer-or-equal to that date.\n        project_path: project root containing _rixpress\n        dry_run: if True, show what would be deleted without deleting\n        timeout_sec: timeout for invoked nix-store commands and for lock staleness checks\n        verbose: if True, print extra diagnostic output\n        ask: if True, prompt for confirmation before destructive operations (default True)\n        pretty: if True, pretty-prints the result (and returns nothing).\n        as_json: if True, pretty prints using json.dumps(indent=2) instead of pprint.\n\n    Returns:\n        A summary dict with canonical keys:\n        kept, deleted, protected, deleted_count, failed_count, referenced_count,\n        log_files_deleted, log_files_failed, dry_run_details\n    \"\"\"\n    nix_bin = shutil.which(\"nix-store\")\n    if not nix_bin:\n        raise FileNotFoundError(\"nix-store not found on PATH. Install Nix or adjust PATH.\")\n\n    project_path = Path(project_path).resolve()\n    if not project_path.exists():\n        raise FileNotFoundError(f\"Project path does not exist: {project_path}\")\n\n    lock_file_path = Path(tempfile.gettempdir()) / \"rixpress_gc.lock\"\n\n    # record of temp gcroot symlink paths we created so we can remove them later\n    created_gcroot_links: List[Path] = []\n\n    # ensure we cleanup on signals\n    def _cleanup_on_signal(signum, frame):\n        logger.info(\"Received signal %s, cleaning up...\", signum)\n        # remove any gcroot links\n        for p in created_gcroot_links:\n            try:\n                if p.exists():\n                    p.unlink()\n            except Exception:\n                pass\n        # remove lock file if held\n        try:\n            if lock_path_context and lock_path_context.acquired:\n                lock_path_context.release()\n        except Exception:\n            pass\n        raise SystemExit(1)\n\n    # placeholder for context so signal handler can access\n    lock_path_context: Optional[LockFile] = None\n\n    # Register handlers\n    old_sigint = signal.getsignal(signal.SIGINT)\n    old_sigterm = signal.getsignal(signal.SIGTERM)\n    signal.signal(signal.SIGINT, _cleanup_on_signal)\n    signal.signal(signal.SIGTERM, _cleanup_on_signal)\n\n    try:\n        # Acquire lock with context manager (atomic)\n        lock_path_context = LockFile(lock_file_path, timeout_sec=timeout_sec)\n        lock_path_context.acquire()\n\n        # parse keep_since\n        if keep_since is not None:\n            if isinstance(keep_since, date) and not isinstance(keep_since, datetime):\n                keep_date = keep_since\n            else:\n                # accept YYYY-MM-DD string\n                try:\n                    keep_date = _parse_iso_date(str(keep_since))\n                except Exception:\n                    raise ValueError(\"Invalid 'keep_since'. Use a date or 'YYYY-MM-DD' string.\")\n        else:\n            keep_date = None\n\n        # Gather logs\n        all_logs = rxp_list_logs(project_path)\n        # Expect list of dicts with 'filename' and 'modification_time'\n        if not isinstance(all_logs, list) or not all_logs:\n            logger.info(\"No build logs found. Nothing to do.\")\n            # canonical empty summary\n            return {\n                \"kept\": [],\n                \"deleted\": [],\n                \"protected\": 0,\n                \"deleted_count\": 0,\n                \"failed_count\": 0,\n                \"referenced_count\": 0,\n                \"log_files_deleted\": 0,\n                \"log_files_failed\": 0,\n                \"dry_run_details\": None,\n            }\n\n        # Partition logs\n        logs_to_keep = []\n        logs_to_delete = []\n        for entry in all_logs:\n            fn = entry.get(\"filename\")\n            mtime = entry.get(\"modification_time\")\n            if not fn or not mtime:\n                continue\n            try:\n                mdate = _parse_iso_date(mtime)\n            except Exception:\n                # If malformed, treat as older than keep_since to be conservative\n                mdate = datetime.min.date()\n            if keep_date is None:\n                logs_to_keep.append(entry)\n            else:\n                if mdate &gt;= keep_date:\n                    logs_to_keep.append(entry)\n                else:\n                    logs_to_delete.append(entry)\n\n        def _filenames(entries: Sequence[Dict]) -&gt; List[str]:\n            return [e[\"filename\"] for e in entries]\n\n        # helper to get store paths per log using rxp_inspect\n        def get_paths_from_logs(filenames: Sequence[str]) -&gt; Dict[str, List[str]]:\n            out: Dict[str, List[str]] = {}\n            for fn in filenames:\n                wl = _extract_which_log(fn)\n                if wl is None:\n                    logger.warning(\"Could not parse which_log from filename: %s\", fn)\n                    out[fn] = []\n                    continue\n                try:\n                    insp_rows = rxp_inspect(project_path=project_path, which_log=wl)\n                except Exception as e:\n                    logger.warning(\"rxp_inspect failed for %s: %s\", fn, e)\n                    out[fn] = []\n                    continue\n                # rxp_inspect returns list of dicts; look for 'path' keys\n                paths = []\n                if isinstance(insp_rows, list):\n                    for row in insp_rows:\n                        if isinstance(row, dict) and \"path\" in row and isinstance(row[\"path\"], str):\n                            paths.append(row[\"path\"])\n                out[fn] = _validate_store_paths(paths)\n            return out\n\n        keep_paths_by_log = get_paths_from_logs(_filenames(logs_to_keep)) if logs_to_keep else {}\n        delete_paths_by_log = get_paths_from_logs(_filenames(logs_to_delete)) if logs_to_delete else {}\n\n        keep_paths_all = _validate_store_paths(sorted({p for lst in keep_paths_by_log.values() for p in lst}))\n        delete_paths_all = _validate_store_paths(sorted({p for lst in delete_paths_by_log.values() for p in lst}))\n\n        summary_info: Dict[str, object] = {\n            \"kept\": _filenames(logs_to_keep),\n            \"deleted\": _filenames(logs_to_delete),\n            \"protected\": 0,\n            \"deleted_count\": 0,\n            \"failed_count\": 0,\n            \"referenced_count\": 0,\n            \"log_files_deleted\": 0,\n            \"log_files_failed\": 0,\n            \"dry_run_details\": None,\n        }\n\n        # DRY RUN branch (date-based)\n        if keep_date is not None and dry_run:\n            logger.info(\"--- DRY RUN --- No changes will be made. ---\")\n            logger.info(\"Logs that would be deleted (%d):\", len(logs_to_delete))\n            for fn in summary_info[\"deleted\"]:\n                logger.info(\"  %s\", fn)\n            details: Dict[str, List[Dict[str, str]]] = {}\n            if delete_paths_by_log:\n                logger.info(\"Artifacts per log (from rxp_inspect):\")\n                for fn, _ in delete_paths_by_log.items():\n                    logger.info(\"== %s ==\", fn)\n                    try:\n                        insp_rows = rxp_inspect(project_path=project_path, which_log=_extract_which_log(fn) or \"\")\n                    except Exception:\n                        logger.info(\"  (rxp_inspect unavailable)\")\n                        details[fn] = []\n                        continue\n                    rows = []\n                    if isinstance(insp_rows, list):\n                        for r in insp_rows:\n                            if not isinstance(r, dict):\n                                continue\n                            rows.append({\"path\": r.get(\"path\", \"\"), \"output\": r.get(\"output\", \"\")})\n                    details[fn] = rows\n            existing_delete_paths = [p for p in delete_paths_all if os.path.exists(p) or os.path.isdir(p)]\n            missing_paths = [p for p in delete_paths_all if p not in existing_delete_paths]\n            logger.info(\"Aggregate store paths targeted for deletion (deduped): %d total, %d existing, %d missing\",\n                        len(delete_paths_all), len(existing_delete_paths), len(missing_paths))\n            if existing_delete_paths:\n                logger.info(\"Existing paths that would be deleted:\")\n                for p in existing_delete_paths:\n                    logger.info(\"  %s\", p)\n            if missing_paths:\n                logger.info(\"Paths already missing (will be skipped):\")\n                for p in missing_paths:\n                    logger.info(\"  %s\", p)\n            summary_info[\"dry_run_details\"] = details\n            if logs_to_delete:\n                logger.info(\"Build log files that would be deleted:\")\n                for fn in summary_info[\"deleted\"]:\n                    log_path = project_path / \"_rixpress\" / fn\n                    exists_indicator = \"[OK]\" if log_path.exists() else \"[X]\"\n                    logger.info(\"  %s %s\", exists_indicator, fn)\n            if pretty:\n                if as_json:\n                    print(json.dumps(summary_info, indent=2, ensure_ascii=False))\n                else:\n                    pprint(summary_info)\n                return\n\n            return summary_info\n\n        # dry-run full GC preview\n        if keep_date is None and dry_run:\n            logger.info(\"--- DRY RUN --- Would run 'nix-store --gc' (delete all unreferenced store paths). ---\")\n            if verbose:\n                logger.info(\"(Tip: for an approximate preview, run 'nix-collect-garbage -n' from a shell.)\")\n            return summary_info\n\n        # Full GC mode\n        if keep_date is None:\n            if ask:\n                proceed = _ask_yes_no(\"Run full Nix garbage collection (delete all unreferenced artifacts)?\", default=False)\n                if not proceed:\n                    logger.info(\"Operation cancelled.\")\n                    return summary_info\n            logger.info(\"Running Nix garbage collector...\")\n            try:\n                _, stdout, stderr = _safe_run([nix_bin, \"--gc\"], timeout=timeout_sec, check=True)\n                if stdout:\n                    if verbose:\n                        logger.info(stdout)\n                    else:\n                        rel = [l for l in stdout.splitlines() if re.search(r\"freed|removing|deleting\", l, re.I)]\n                        if rel:\n                            for line in rel[-10:]:\n                                logger.info(line)\n                logger.info(\"Garbage collection complete.\")\n                return summary_info\n            except RxpGCError as e:\n                raise\n\n        # Targeted deletion mode\n        if not logs_to_delete:\n            logger.info(\"No build logs older than %s found. Nothing to do.\", keep_date.isoformat())\n            return summary_info\n\n        if not delete_paths_all:\n            logger.info(\"No valid store paths found in logs older than %s. Nothing to delete.\", keep_date.isoformat())\n            return summary_info\n\n        prompt = f\"This will permanently delete {len(delete_paths_all)} store paths from {len(logs_to_delete)} build(s) older than {keep_date.isoformat()}. Continue?\"\n        if ask:\n            if not _ask_yes_no(prompt, default=False):\n                logger.info(\"Operation cancelled.\")\n                return summary_info\n\n        # Protect recent artifacts (date-based mode only) by adding indirect GC roots.\n        temp_gcroots_dir: Optional[Path] = None\n        protected = 0\n        try:\n            if keep_paths_all:\n                temp_gcroots_dir = Path(tempfile.mkdtemp(prefix=\"rixpress-gc-\"))\n                logger.info(\"Protecting %d recent artifacts via GC roots...\", len(keep_paths_all))\n                for i, p in enumerate(keep_paths_all, start=1):\n                    link_path = temp_gcroots_dir / f\"root-{i}\"\n                    try:\n                        # create a placeholder link path (the nix-store --add-root will create the gcroot)\n                        # use link_path as the path to register the indirect root\n                        _safe_run([nix_bin, \"--add-root\", str(link_path), \"--indirect\", p], timeout=timeout_sec, check=True)\n                        created_gcroot_links.append(link_path)\n                        protected += 1\n                    except RxpGCError as e:\n                        logger.warning(\"Failed to add GC root for %s: %s\", p, e)\n                if protected == 0:\n                    raise RxpGCError(\"Failed to protect any store paths. Aborting.\")\n                summary_info[\"protected\"] = protected\n\n            # Delete specific store paths\n            logger.info(\"Deleting %d targeted store paths...\", len(delete_paths_all))\n            existing_paths = [p for p in delete_paths_all if os.path.exists(p) or os.path.isdir(p)]\n            missing_paths = [p for p in delete_paths_all if p not in existing_paths]\n            if missing_paths:\n                logger.info(\"Skipping %d paths that no longer exist.\", len(missing_paths))\n                if verbose:\n                    for p in missing_paths:\n                        logger.info(\"  Missing: %s\", p)\n            if not existing_paths:\n                logger.info(\"No existing paths to delete. All targeted paths are already gone.\")\n                return summary_info\n\n            total_deleted = 0\n            failed_paths: List[str] = []\n            referenced_paths: List[str] = []\n\n            for i, pth in enumerate(existing_paths, start=1):\n                if not (os.path.exists(pth) or os.path.isdir(pth)):\n                    logger.info(\"  [%d/%d] Skipping %s (already gone)\", i, len(existing_paths), os.path.basename(pth))\n                    continue\n                logger.info(\"  [%d/%d] Attempting to delete %s...\", i, len(existing_paths), os.path.basename(pth))\n                try:\n                    proc = subprocess.run([nix_bin, \"--delete\", pth], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=timeout_sec)\n                    out = (proc.stdout or \"\") + \"\\n\" + (proc.stderr or \"\")\n                    if proc.returncode == 0:\n                        total_deleted += 1\n                        logger.info(\"    [OK] Successfully deleted\")\n                        if verbose and out.strip():\n                            logger.info(\"    %s\", out.strip())\n                    else:\n                        if re.search(r\"still alive|Cannot delete\", out, re.I):\n                            referenced_paths.append(pth)\n                            logger.info(\"    [!] Skipped (still referenced)\")\n                            if verbose:\n                                logger.info(\"    Details: %s\", out.strip())\n                        else:\n                            failed_paths.append(pth)\n                            logger.info(\"    [X] Failed to delete\")\n                            if verbose:\n                                logger.info(\"    Details: %s\", out.strip())\n                except subprocess.TimeoutExpired:\n                    failed_paths.append(pth)\n                    logger.info(\"    [X] Timeout while deleting\")\n                except Exception as e:\n                    failed_paths.append(pth)\n                    logger.info(\"    [X] Error: %s\", e)\n\n            # Summary of deletion\n            logger.info(\"\\nDeletion summary:\")\n            logger.info(\"  Successfully deleted: %d paths\", total_deleted)\n            logger.info(\"  Skipped (still referenced): %d paths\", len(referenced_paths))\n            logger.info(\"  Failed (other errors): %d paths\", len(failed_paths))\n\n            if referenced_paths and verbose:\n                logger.info(\"\\nReferenced paths (cannot delete):\")\n                for pth in referenced_paths:\n                    logger.info(\"  %s\", os.path.basename(pth))\n                    try:\n                        _, roots_out, _ = _safe_run([nix_bin, \"--query\", \"--roots\", pth], timeout=timeout_sec, check=False)\n                        if roots_out.strip():\n                            logger.info(\"    GC roots: %s\", roots_out.strip().replace(\"\\n\", \", \"))\n                        else:\n                            logger.info(\"    GC roots: (none found)\")\n                    except Exception:\n                        logger.info(\"    GC roots: (query failed)\")\n                    try:\n                        _, refs_out, _ = _safe_run([nix_bin, \"--query\", \"--referrers\", pth], timeout=timeout_sec, check=False)\n                        if refs_out.strip():\n                            refs = [os.path.basename(x) for x in refs_out.splitlines() if x.strip()]\n                            logger.info(\"    Referenced by: %s\", \", \".join(refs) if refs else \"(none)\")\n                        else:\n                            logger.info(\"    Referenced by: (none)\")\n                    except Exception:\n                        logger.info(\"    Referenced by: (query failed)\")\n\n            summary_info[\"deleted_count\"] = total_deleted\n            summary_info[\"failed_count\"] = len(failed_paths)\n            summary_info[\"referenced_count\"] = len(referenced_paths)\n\n            # Delete old build log files\n            if logs_to_delete:\n                logger.info(\"\\nDeleting old build log files...\")\n                log_files_deleted = 0\n                log_files_failed: List[str] = []\n                for i, entry in enumerate(logs_to_delete, start=1):\n                    log_file = entry[\"filename\"]\n                    log_path = project_path / \"_rixpress\" / log_file\n                    logger.info(\"  [%d/%d] Deleting %s...\", i, len(logs_to_delete), log_file)\n                    if not log_path.exists():\n                        logger.info(\"    [!] File not found (already deleted?)\")\n                        continue\n                    try:\n                        log_path.unlink()\n                        if not log_path.exists():\n                            log_files_deleted += 1\n                            logger.info(\"    [OK] Successfully deleted\")\n                        else:\n                            log_files_failed.append(log_file)\n                            logger.info(\"    [X] Failed to delete (file still exists)\")\n                    except Exception as e:\n                        log_files_failed.append(log_file)\n                        logger.info(\"    [X] Error: %s\", e)\n                logger.info(\"\\nBuild log deletion summary:\")\n                logger.info(\"  Successfully deleted: %d files\", log_files_deleted)\n                logger.info(\"  Failed: %d files\", len(log_files_failed))\n                if log_files_failed and verbose:\n                    logger.info(\"\\nFailed to delete log files:\")\n                    for lf in log_files_failed:\n                        logger.info(\"  %s\", lf)\n                summary_info[\"log_files_deleted\"] = log_files_deleted\n                summary_info[\"log_files_failed\"] = len(log_files_failed)\n\n            logger.info(\"\\nCleanup complete!\")\n            return summary_info\n        finally:\n            # Always attempt to remove created gcroot links and the temp dir\n            if created_gcroot_links:\n                for p in created_gcroot_links:\n                    try:\n                        if p.exists():\n                            p.unlink()\n                    except Exception:\n                        logger.debug(\"Failed to unlink gcroot link %s\", p)\n                # attempt to remove the parent temp directory if exists and empty\n                if temp_gcroots_dir and temp_gcroots_dir.exists():\n                    try:\n                        shutil.rmtree(temp_gcroots_dir)\n                    except Exception:\n                        # ignore: best-effort cleanup\n                        logger.debug(\"Failed to remove temp gcroots dir %s\", temp_gcroots_dir)\n    finally:\n        # always release lock and restore signals\n        try:\n            if lock_path_context is not None:\n                lock_path_context.release()\n        except Exception:\n            pass\n        try:\n            signal.signal(signal.SIGINT, old_sigint)\n            signal.signal(signal.SIGTERM, old_sigterm)\n        except Exception:\n            pass\n</code></pre>"}]}