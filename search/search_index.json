{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ryxpress","text":"<p>Welcome to the documentation for ryxpress.</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>TBC</p>"},{"location":"#api","title":"API","text":"<p>You can view autogenerated API reference under the \"API \u2192 Reference\" page.</p>"},{"location":"reference/","title":"Reference","text":"<p>Run the rixpress R pipeline (rxp_populate + rxp_make) by sourcing an R script.</p> <p>Parameters - script: Path or name of the R script to run (defaults to \"gen-pipeline.R\").           If a relative path is given and doesn't exist in the working directory,           this function will attempt to locate the script on PATH. - verbose: integer passed to rixpress::rxp_make(verbose = ...) - max_jobs: integer passed to rixpress::rxp_make(max_jobs = ...) - cores: integer passed to rixpress::rxp_make(cores = ...) - rscript_cmd: the Rscript binary to use (defaults to \"Rscript\") - timeout: optional timeout in seconds for the subprocess.run call - cwd: optional working directory to run Rscript in. If None, the directory        containing the provided script will be used. This is important because        pipeline.nix and related files are often imported with relative paths        (e.g. ./default.nix), so Rscript needs to be run where those files are reachable.</p> <p>Returns an RRunResult containing returncode, stdout, stderr.</p> <p>Security note: this will execute arbitrary R code. Only run trusted scripts.</p> Source code in <code>src/ryxpress/r_runner.py</code> <pre><code>def rxp_make(\n    script: Union[str, Path] = \"gen-pipeline.R\",\n    verbose: int = 0,\n    max_jobs: int = 1,\n    cores: int = 1,\n    rscript_cmd: str = \"Rscript\",\n    timeout: Optional[int] = None,\n    cwd: Optional[Union[str, Path]] = None,\n) -&gt; RRunResult:\n    \"\"\"\n    Run the rixpress R pipeline (rxp_populate + rxp_make) by sourcing an R script.\n\n    Parameters\n    - script: Path or name of the R script to run (defaults to \"gen-pipeline.R\").\n              If a relative path is given and doesn't exist in the working directory,\n              this function will attempt to locate the script on PATH.\n    - verbose: integer passed to rixpress::rxp_make(verbose = ...)\n    - max_jobs: integer passed to rixpress::rxp_make(max_jobs = ...)\n    - cores: integer passed to rixpress::rxp_make(cores = ...)\n    - rscript_cmd: the Rscript binary to use (defaults to \"Rscript\")\n    - timeout: optional timeout in seconds for the subprocess.run call\n    - cwd: optional working directory to run Rscript in. If None, the directory\n           containing the provided script will be used. This is important because\n           pipeline.nix and related files are often imported with relative paths\n           (e.g. ./default.nix), so Rscript needs to be run where those files are reachable.\n\n    Returns an RRunResult containing returncode, stdout, stderr.\n\n    Security note: this will execute arbitrary R code. Only run trusted scripts.\n    \"\"\"\n    # Validate integers\n    for name, val in ((\"verbose\", verbose), (\"max_jobs\", max_jobs), (\"cores\", cores)):\n        if not isinstance(val, int):\n            raise TypeError(f\"{name} must be an int, got {type(val).__name__}\")\n        if val &lt; 0:\n            raise ValueError(f\"{name} must be &gt;= 0\")\n\n    # Resolve script path: prefer given path if it exists; otherwise try to find on PATH\n    script_path = Path(script)\n    if not script_path.is_file():\n        # If a bare name was provided, attempt to find it on PATH\n        found = shutil.which(str(script))\n        if found:\n            script_path = Path(found)\n        else:\n            raise FileNotFoundError(\n                f\"R script '{script}' not found in working directory and not on PATH\"\n            )\n    else:\n        script_path = script_path.resolve()\n\n    # Determine working directory for the R process:\n    if cwd is not None:\n        run_cwd = Path(cwd).resolve()\n        if not run_cwd.is_dir():\n            raise FileNotFoundError(f\"Requested cwd '{cwd}' does not exist or is not a directory\")\n    else:\n        # default to the script's parent directory so relative imports (./default.nix) work\n        run_cwd = script_path.parent\n\n    # Verify Rscript binary exists\n    if shutil.which(rscript_cmd) is None:\n        raise FileNotFoundError(\n            f\"Rscript binary '{rscript_cmd}' not found in PATH. Ensure R is installed or adjust rscript_cmd.\"\n        )\n\n    # Prepare wrapper R script that:\n    #  - loads rixpress,\n    #  - sources the user's script,\n    #  - if the sourced evaluation returns a list, calls rxp_populate on it,\n    #  - then calls rixpress::rxp_make(...) with the provided args.\n    wrapper = f\"\"\"\nsuppressPackageStartupMessages(library(rixpress))\n\nscript_path &lt;- \"{script_path.as_posix()}\"\n\nif (!file.exists(script_path)) {{\n  stop(\"Script not found: \", script_path)\n}}\n\nresult_value &lt;- NULL\n\nres &lt;- tryCatch({{\n  # Source &amp; evaluate the user's script and capture the returned value (if any)\n  result_value &lt;- eval(parse(script_path))\n  # If the script returned a list (a pipeline), run rxp_populate on it\n  if (!is.null(result_value) &amp;&amp; is.list(result_value)) {{\n    pipeline &lt;- result_value\n    pipeline &lt;- rixpress::rxp_populate(pipeline)\n  }}\n  # Finally, run rxp_make with the given integer parameters\n  rixpress::rxp_make(\n    verbose = {int(verbose)},\n    max_jobs = {int(max_jobs)},\n    cores = {int(cores)}\n  )\n}}, error = function(e) {{\n  # Print a clear error message and exit with non-zero status\n  message(\"rixpress-python-runner-error: \", conditionMessage(e))\n  quit(status = 1)\n}})\n\n# If we reach here, exit with success\nquit(status = 0)\n\"\"\"\n\n    # Create temporary file for wrapper\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".R\", delete=False) as tf:\n        tf.write(wrapper)\n        wrapper_path = Path(tf.name)\n\n    try:\n        # Run Rscript on the wrapper file using the desired working directory\n        proc = subprocess.run(\n            [rscript_cmd, str(wrapper_path)],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=timeout,\n            cwd=str(run_cwd),\n        )\n        return RRunResult(returncode=proc.returncode, stdout=proc.stdout, stderr=proc.stderr)\n    finally:\n        try:\n            wrapper_path.unlink()\n        except Exception:\n            pass\n</code></pre> <p>Copy derivations from the Nix store to the current working directory.</p> <p>Translation of R's rxp_copy to a dependency-free Python function.</p> <p>Behavior: - Uses rxp_inspect to read the most recent build log and find derivation   entries and their store paths. - Copies the outputs of a single derivation (or the special \"all-derivations\")   into ./pipeline-output (created if necessary). - Applies POSIX permission modes to directories and files (dir_mode/file_mode   are octal strings like \"0755\" or \"755\"). - Returns None; raises exceptions on errors.</p> <p>Garbage collect rixpress build artifacts and logs.</p> <p>Improved translation of the R function rxp_gc to Python with robust cleanup: - Atomic lock file creation to avoid races - Signal handlers (SIGINT/SIGTERM) to ensure cleanup on interruption - Temporary GC roots are recorded and removed after the operation (so they   don't keep artifacts alive forever) - 'force' parameter to skip interactive confirmation (useful for CI) - Summary dict always contains canonical keys - Dependency-free (standard library only)</p> <p>Initialize an rixpress project by creating gen-env.R and gen-pipeline.R.</p> <p>This is a direct translation of the R function <code>rxp_init</code> into Python.</p> <p>Behavior: - Creates (overwrites) two files in the project directory:   - gen-env.R   - gen-pipeline.R - Asks the user for confirmation before making changes, unless skip_prompt=True. - Creates the project directory if it does not exist. - If running non-interactively (e.g. CI), skips Git initialization and returns True. - If the user agrees to initialise git, attempts to run <code>git init</code> in the project directory.   If <code>git</code> is not available it prints an informative message. - Returns True on successful initialization, False if the user cancels before making changes.</p> <p>Dependency-free utilities to list and inspect rixpress build logs.</p> <p>Behavior: - No external dependencies (no pandas). - rxp_list_logs returns a list of dicts with keys:     - filename (str)     - modification_time (YYYY-MM-DD string)     - size_kb (float)   ordered most recent first. - rxp_inspect selects a log (most recent or regex match) and returns the JSON   content coerced into a list-of-dicts (rows). - Errors: raises FileNotFoundError when _rixpress or logs are missing;   raises ValueError when which_log is provided but no match is found. - Uses the standard library logging module to emit an INFO message when a   specific log is chosen via which_log.</p> <p>This mirrors the R functions rxp_list_logs and rxp_inspect as closely as possible while being dependency-free and returning plain Python structures.</p> <p>Helpers to read/load artifacts from the Nix store for ryxpress.</p> <p>Behavior: - Resolve derivation outputs (single path or list of paths) via rxp_inspect   or by accepting a literal /nix/store/... path. - When a single file is resolved:   - Try to unpickle the file first (regardless of extension). If that succeeds,     return the loaded object.   - Otherwise try to load using rds2py (if available) and return that object.   - If neither works, return the path (string) \u2014 do not raise or warn. - If multiple outputs are resolved, return the list of paths. - The functions intentionally avoid raising errors or emitting warnings for   normal \"can't load this artifact\" cases; they prefer to return the path(s).</p> <p>Export pipeline DAG for CI and prepare node/edge data.</p> <p>This module provides two functions translated from the original R code:</p> <ul> <li> <p>get_nodes_edges(path_dag=\"_rixpress/dag.json\")     Reads the pipeline DAG JSON produced by rxp_populate and returns a dict     with 'nodes' and 'edges' lists suitable for further processing.</p> </li> <li> <p>rxp_dag_for_ci(nodes_and_edges=None, output_file=\"_rixpress/dag.dot\")     Uses python-igraph to build a graph from the nodes/edges and writes a DOT     file to output_file. Raises ImportError if python-igraph is not available.</p> </li> </ul> <p>Notes: - This implementation is defensive about JSON shape: it tolerates derivation   entries where fields may be scalars or lists, and normalizes them. - The DOT writer expects the python 'igraph' package (python-igraph). If it is   not importable, rxp_dag_for_ci raises ImportError with a clear message.</p>"},{"location":"reference/#ryxpress.rxp_copy.rxp_copy","title":"<code>rxp_copy(derivation_name=None, dir_mode='0755', file_mode='0644', project_path='.')</code>","text":"<p>Copy derivations from the Nix store to ./pipeline-output.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>derivation_name</code> <p>name of the derivation to copy (string). If None, uses the special derivation name \"all-derivations\" (mirrors R).</p> required <code>-</code> <code>dir_mode / file_mode</code> <p>octal permission strings applied to copied dirs/files.</p> required <code>-</code> <code>project_path</code> <p>project root where _rixpress lives (defaults to \".\").</p> required Source code in <code>src/ryxpress/rxp_copy.py</code> <pre><code>def rxp_copy(\n    derivation_name: Optional[str] = None,\n    dir_mode: str = \"0755\",\n    file_mode: str = \"0644\",\n    project_path: Union[str, Path] = \".\",\n) -&gt; None:\n    \"\"\"\n    Copy derivations from the Nix store to ./pipeline-output.\n\n    Parameters:\n      - derivation_name: name of the derivation to copy (string). If None,\n        uses the special derivation name \"all-derivations\" (mirrors R).\n      - dir_mode / file_mode: octal permission strings applied to copied dirs/files.\n      - project_path: project root where _rixpress lives (defaults to \".\").\n\n    Raises:\n      - FileNotFoundError if _rixpress or logs are missing.\n      - ValueError on invalid modes or derivation not found.\n      - RuntimeError on copy failures.\n    \"\"\"\n    project = Path(project_path)\n    # Validate modes\n    if not _valid_mode(dir_mode):\n        raise ValueError('Invalid dir_mode: provide a character octal like \"0755\" or \"755\".')\n    if not _valid_mode(file_mode):\n        raise ValueError('Invalid file_mode: provide a character octal like \"0644\" or \"644\".')\n\n    # Ensure there is a build log\n    logs = rxp_list_logs(project)\n    # rxp_list_logs raises if none; if it returned, we have log entries\n\n    # Read latest build log content via rxp_inspect (most recent)\n    rows = rxp_inspect(project_path=project, which_log=None)\n    if not isinstance(rows, list) or not rows:\n        raise RuntimeError(\"Could not read build log details; rxp_inspect returned no rows.\")\n\n    # Build a mapping from derivation name -&gt; list of store paths\n    # We try to be tolerant: look for keys 'derivation' (R), then 'deriv', 'name'\n    deriv_key_candidates = (\"derivation\", \"deriv\", \"name\")\n    path_key_candidates = (\"path\", \"store_path\", \"path_store\", \"output_path\", \"output\")\n\n    deriv_to_paths: Dict[str, List[str]] = {}\n    for r in rows:\n        if not isinstance(r, dict):\n            continue\n        deriv_val = _extract_field(r, deriv_key_candidates)\n        path_val = _extract_field(r, path_key_candidates)\n        if deriv_val is None:\n            # skip rows without a derivation name\n            continue\n        derivs = _ensure_iterable_of_strings(deriv_val)\n        paths = _ensure_iterable_of_strings(path_val)\n        for d in derivs:\n            deriv_to_paths.setdefault(d, []).extend(paths)\n\n    # Deduplicate path lists\n    for k in list(deriv_to_paths.keys()):\n        seen = []\n        for p in deriv_to_paths[k]:\n            if p not in seen:\n                seen.append(p)\n        deriv_to_paths[k] = seen\n\n    # Choose derivation_name if not provided\n    if derivation_name is None:\n        derivation_name = \"all-derivations\"\n\n    if derivation_name not in deriv_to_paths:\n        # Provide hint of available derivations (up to 20)\n        available = list(deriv_to_paths.keys())[:20]\n        more = \", ...\" if len(deriv_to_paths) &gt; 20 else \"\"\n        raise ValueError(\n            f\"No derivation {derivation_name!r} found in the build log. Available: {', '.join(available)}{more}\"\n        )\n\n    # Collect paths for this derivation\n    deriv_paths = deriv_to_paths.get(derivation_name, [])\n    if not deriv_paths:\n        raise RuntimeError(f\"No store paths recorded for derivation {derivation_name!r} in the build log.\")\n\n    output_dir = _ensure_output_dir(Path.cwd())\n\n    # For each store path, copy its contents into output_dir\n    copy_failed = False\n    errors: List[str] = []\n    for store_path_str in deriv_paths:\n        store_path = Path(store_path_str)\n        if not store_path.exists():\n            # Skip non-existing path (warn)\n            logger.warning(\"Store path does not exist, skipping: %s\", store_path)\n            continue\n        try:\n            # If the derivation path is a directory, copy its children into output_dir\n            if store_path.is_dir():\n                # copy each child into output_dir, preserving names\n                for child in store_path.iterdir():\n                    dest = output_dir / child.name\n                    if child.is_dir():\n                        # Python 3.8+: dirs_exist_ok True will merge\n                        try:\n                            shutil.copytree(child, dest, dirs_exist_ok=True)\n                        except TypeError:\n                            # older Python: fallback to manual merge\n                            if dest.exists():\n                                # copy contents into existing dest\n                                for sub in child.rglob(\"*\"):\n                                    rel = sub.relative_to(child)\n                                    target = dest / rel\n                                    if sub.is_dir():\n                                        target.mkdir(parents=True, exist_ok=True)\n                                    else:\n                                        target.parent.mkdir(parents=True, exist_ok=True)\n                                        shutil.copy2(sub, target)\n                            else:\n                                shutil.copytree(child, dest)\n                    else:\n                        # file: copy, possibly overwrite\n                        shutil.copy2(child, dest)\n            else:\n                # store_path is a file: copy into output_dir\n                dest_file = output_dir / store_path.name\n                shutil.copy2(store_path, dest_file)\n        except Exception as e:\n            copy_failed = True\n            errors.append(f\"{store_path}: {e}\")\n            logger.debug(\"Copy error for %s: %s\", store_path, e)\n\n    # Apply permissions\n    try:\n        _apply_permissions(output_dir, dir_mode=dir_mode, file_mode=file_mode)\n    except Exception:\n        # Best-effort: ignore permission application errors\n        logger.debug(\"Failed to apply permissions to %s\", output_dir)\n\n    if copy_failed:\n        raise RuntimeError(f\"Copy unsuccessful: errors occurred:\\n\" + \"\\n\".join(errors))\n\n    # Success message\n    print(f\"Copy successful, check out {output_dir}\")\n    return None\n</code></pre>"},{"location":"reference/#ryxpress.rxp_gc.LockFile","title":"<code>LockFile</code>","text":"<p>Context manager for atomic lock file with basic staleness handling.</p> Source code in <code>src/ryxpress/rxp_gc.py</code> <pre><code>class LockFile:\n    \"\"\"Context manager for atomic lock file with basic staleness handling.\"\"\"\n\n    def __init__(self, path: Union[str, Path], timeout_sec: int = 300):\n        self.path = Path(path)\n        self.timeout_sec = timeout_sec\n        self.acquired = False\n        self.fd = None\n\n    def _write_lock(self):\n        now = datetime.now().isoformat()\n        pid = os.getpid()\n        # Write using the file descriptor we created atomically\n        os.write(self.fd, f\"{pid}\\n{now}\\n\".encode(\"utf-8\"))\n        os.fsync(self.fd)\n\n    def _is_stale(self, timestamp_str: str) -&gt; bool:\n        try:\n            ts = datetime.fromisoformat(timestamp_str)\n        except Exception:\n            return True\n        return (datetime.now() - ts).total_seconds() &gt; self.timeout_sec\n\n    def acquire(self):\n        # Try atomic creation\n        flags = os.O_CREAT | os.O_EXCL | os.O_WRONLY\n        mode = 0o644\n        try:\n            self.fd = os.open(str(self.path), flags, mode)\n            self._write_lock()\n            self.acquired = True\n            return True\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n            # Lock file exists: examine it for staleness\n            try:\n                content = self.path.read_text().splitlines()\n                if len(content) &gt;= 2:\n                    try:\n                        pid = int(content[0])\n                    except Exception:\n                        pid = None\n                    tstamp = content[1]\n                else:\n                    pid = None\n                    tstamp = None\n            except Exception:\n                pid = None\n                tstamp = None\n\n            # If a PID is present and exists on POSIX, consider it alive\n            alive = False\n            if pid is not None and os.name == \"posix\":\n                try:\n                    os.kill(pid, 0)\n                    alive = True\n                except OSError:\n                    alive = False\n            # If alive and recent, fail\n            if alive and tstamp is not None:\n                try:\n                    ts = datetime.fromisoformat(tstamp)\n                    if (datetime.now() - ts).total_seconds() &lt;= self.timeout_sec:\n                        raise RxpGCError(f\"Another rxp_gc process appears to be running (PID: {pid}). If not, remove the lock: {self.path}\")\n                except ValueError:\n                    # can't parse timestamp: be conservative and fail\n                    raise RxpGCError(f\"Lock file present and appears active: {self.path}\")\n\n            # If not alive or lock is stale, remove it and retry atomic creation once\n            try:\n                self.path.unlink()\n            except Exception:\n                raise RxpGCError(f\"Could not remove stale lock file: {self.path}\")\n            # Retry atomic creation\n            try:\n                self.fd = os.open(str(self.path), flags, mode)\n                self._write_lock()\n                self.acquired = True\n                return True\n            except OSError as e2:\n                raise RxpGCError(f\"Failed to create lock file: {e2}\") from e2\n\n    def release(self):\n        if self.acquired:\n            try:\n                if self.fd is not None:\n                    os.close(self.fd)\n            except Exception:\n                pass\n            try:\n                if self.path.exists():\n                    self.path.unlink()\n            except Exception:\n                pass\n            self.acquired = False\n\n    def __enter__(self):\n        self.acquire()\n        return self\n\n    def __exit__(self, exc_type, exc, tb):\n        self.release()\n</code></pre>"},{"location":"reference/#ryxpress.rxp_gc.rxp_gc","title":"<code>rxp_gc(keep_since=None, project_path='.', dry_run=False, timeout_sec=300, verbose=False, force=False)</code>","text":"<p>Garbage collect Nix store paths and build logs produced by rixpress.</p> <p>Parameters - keep_since: None for full GC, or a date/ISO date string (YYYY-MM-DD) to keep logs newer-or-equal to that date. - project_path: project root containing _rixpress - dry_run: if True, show what would be deleted without deleting - timeout_sec: timeout for invoked nix-store commands and for lock staleness checks - verbose: if True, print extra diagnostic output - force: if True, skip interactive confirmations (useful for CI)</p> <p>Returns:</p> Type Description <code>Dict[str, object]</code> <p>A summary dict with canonical keys: kept, deleted, protected, deleted_count, failed_count, referenced_count, log_files_deleted, log_files_failed, dry_run_details</p> Source code in <code>src/ryxpress/rxp_gc.py</code> <pre><code>def rxp_gc(\n    keep_since: Optional[Union[str, date]] = None,\n    project_path: Union[str, Path] = \".\",\n    dry_run: bool = False,\n    timeout_sec: int = 300,\n    verbose: bool = False,\n    force: bool = False,\n) -&gt; Dict[str, object]:\n    \"\"\"\n    Garbage collect Nix store paths and build logs produced by rixpress.\n\n    Parameters\n    - keep_since: None for full GC, or a date/ISO date string (YYYY-MM-DD) to keep logs newer-or-equal to that date.\n    - project_path: project root containing _rixpress\n    - dry_run: if True, show what would be deleted without deleting\n    - timeout_sec: timeout for invoked nix-store commands and for lock staleness checks\n    - verbose: if True, print extra diagnostic output\n    - force: if True, skip interactive confirmations (useful for CI)\n\n    Returns:\n      A summary dict with canonical keys:\n        kept, deleted, protected, deleted_count, failed_count, referenced_count,\n        log_files_deleted, log_files_failed, dry_run_details\n    \"\"\"\n    nix_bin = shutil.which(\"nix-store\")\n    if not nix_bin:\n        raise FileNotFoundError(\"nix-store not found on PATH. Install Nix or adjust PATH.\")\n\n    project_path = Path(project_path).resolve()\n    if not project_path.exists():\n        raise FileNotFoundError(f\"Project path does not exist: {project_path}\")\n\n    lock_file_path = Path(tempfile.gettempdir()) / \"rixpress_gc.lock\"\n\n    # record of temp gcroot symlink paths we created so we can remove them later\n    created_gcroot_links: List[Path] = []\n\n    # ensure we cleanup on signals\n    def _cleanup_on_signal(signum, frame):\n        logger.info(\"Received signal %s, cleaning up...\", signum)\n        # remove any gcroot links\n        for p in created_gcroot_links:\n            try:\n                if p.exists():\n                    p.unlink()\n            except Exception:\n                pass\n        # remove lock file if held\n        try:\n            if lock_path_context and lock_path_context.acquired:\n                lock_path_context.release()\n        except Exception:\n            pass\n        raise SystemExit(1)\n\n    # placeholder for context so signal handler can access\n    lock_path_context: Optional[LockFile] = None\n\n    # Register handlers\n    old_sigint = signal.getsignal(signal.SIGINT)\n    old_sigterm = signal.getsignal(signal.SIGTERM)\n    signal.signal(signal.SIGINT, _cleanup_on_signal)\n    signal.signal(signal.SIGTERM, _cleanup_on_signal)\n\n    try:\n        # Acquire lock with context manager (atomic)\n        lock_path_context = LockFile(lock_file_path, timeout_sec=timeout_sec)\n        lock_path_context.acquire()\n\n        # parse keep_since\n        if keep_since is not None:\n            if isinstance(keep_since, date) and not isinstance(keep_since, datetime):\n                keep_date = keep_since\n            else:\n                # accept YYYY-MM-DD string\n                try:\n                    keep_date = _parse_iso_date(str(keep_since))\n                except Exception:\n                    raise ValueError(\"Invalid 'keep_since'. Use a date or 'YYYY-MM-DD' string.\")\n        else:\n            keep_date = None\n\n        # Gather logs\n        all_logs = rxp_list_logs(project_path)\n        # Expect list of dicts with 'filename' and 'modification_time'\n        if not isinstance(all_logs, list) or not all_logs:\n            logger.info(\"No build logs found. Nothing to do.\")\n            # canonical empty summary\n            return {\n                \"kept\": [],\n                \"deleted\": [],\n                \"protected\": 0,\n                \"deleted_count\": 0,\n                \"failed_count\": 0,\n                \"referenced_count\": 0,\n                \"log_files_deleted\": 0,\n                \"log_files_failed\": 0,\n                \"dry_run_details\": None,\n            }\n\n        # Partition logs\n        logs_to_keep = []\n        logs_to_delete = []\n        for entry in all_logs:\n            fn = entry.get(\"filename\")\n            mtime = entry.get(\"modification_time\")\n            if not fn or not mtime:\n                continue\n            try:\n                mdate = _parse_iso_date(mtime)\n            except Exception:\n                # If malformed, treat as older than keep_since to be conservative\n                mdate = datetime.min.date()\n            if keep_date is None:\n                logs_to_keep.append(entry)\n            else:\n                if mdate &gt;= keep_date:\n                    logs_to_keep.append(entry)\n                else:\n                    logs_to_delete.append(entry)\n\n        def _filenames(entries: Sequence[Dict]) -&gt; List[str]:\n            return [e[\"filename\"] for e in entries]\n\n        # helper to get store paths per log using rxp_inspect\n        def get_paths_from_logs(filenames: Sequence[str]) -&gt; Dict[str, List[str]]:\n            out: Dict[str, List[str]] = {}\n            for fn in filenames:\n                wl = _extract_which_log(fn)\n                if wl is None:\n                    logger.warning(\"Could not parse which_log from filename: %s\", fn)\n                    out[fn] = []\n                    continue\n                try:\n                    insp_rows = rxp_inspect(project_path=project_path, which_log=wl)\n                except Exception as e:\n                    logger.warning(\"rxp_inspect failed for %s: %s\", fn, e)\n                    out[fn] = []\n                    continue\n                # rxp_inspect returns list of dicts; look for 'path' keys\n                paths = []\n                if isinstance(insp_rows, list):\n                    for row in insp_rows:\n                        if isinstance(row, dict) and \"path\" in row and isinstance(row[\"path\"], str):\n                            paths.append(row[\"path\"])\n                out[fn] = _validate_store_paths(paths)\n            return out\n\n        keep_paths_by_log = get_paths_from_logs(_filenames(logs_to_keep)) if logs_to_keep else {}\n        delete_paths_by_log = get_paths_from_logs(_filenames(logs_to_delete)) if logs_to_delete else {}\n\n        keep_paths_all = _validate_store_paths(sorted({p for lst in keep_paths_by_log.values() for p in lst}))\n        delete_paths_all = _validate_store_paths(sorted({p for lst in delete_paths_by_log.values() for p in lst}))\n\n        summary_info: Dict[str, object] = {\n            \"kept\": _filenames(logs_to_keep),\n            \"deleted\": _filenames(logs_to_delete),\n            \"protected\": 0,\n            \"deleted_count\": 0,\n            \"failed_count\": 0,\n            \"referenced_count\": 0,\n            \"log_files_deleted\": 0,\n            \"log_files_failed\": 0,\n            \"dry_run_details\": None,\n        }\n\n        # DRY RUN branch (date-based)\n        if keep_date is not None and dry_run:\n            logger.info(\"--- DRY RUN --- No changes will be made. ---\")\n            logger.info(\"Logs that would be deleted (%d):\", len(logs_to_delete))\n            for fn in summary_info[\"deleted\"]:\n                logger.info(\"  %s\", fn)\n            details: Dict[str, List[Dict[str, str]]] = {}\n            if delete_paths_by_log:\n                logger.info(\"Artifacts per log (from rxp_inspect):\")\n                for fn, _ in delete_paths_by_log.items():\n                    logger.info(\"== %s ==\", fn)\n                    try:\n                        insp_rows = rxp_inspect(project_path=project_path, which_log=_extract_which_log(fn) or \"\")\n                    except Exception:\n                        logger.info(\"  (rxp_inspect unavailable)\")\n                        details[fn] = []\n                        continue\n                    rows = []\n                    if isinstance(insp_rows, list):\n                        for r in insp_rows:\n                            if not isinstance(r, dict):\n                                continue\n                            rows.append({\"path\": r.get(\"path\", \"\"), \"output\": r.get(\"output\", \"\")})\n                    details[fn] = rows\n            existing_delete_paths = [p for p in delete_paths_all if os.path.exists(p) or os.path.isdir(p)]\n            missing_paths = [p for p in delete_paths_all if p not in existing_delete_paths]\n            logger.info(\"Aggregate store paths targeted for deletion (deduped): %d total, %d existing, %d missing\",\n                        len(delete_paths_all), len(existing_delete_paths), len(missing_paths))\n            if existing_delete_paths:\n                logger.info(\"Existing paths that would be deleted:\")\n                for p in existing_delete_paths:\n                    logger.info(\"  %s\", p)\n            if missing_paths:\n                logger.info(\"Paths already missing (will be skipped):\")\n                for p in missing_paths:\n                    logger.info(\"  %s\", p)\n            summary_info[\"dry_run_details\"] = details\n            if logs_to_delete:\n                logger.info(\"Build log files that would be deleted:\")\n                for fn in summary_info[\"deleted\"]:\n                    log_path = project_path / \"_rixpress\" / fn\n                    exists_indicator = \"[OK]\" if log_path.exists() else \"[X]\"\n                    logger.info(\"  %s %s\", exists_indicator, fn)\n            return summary_info\n\n        # dry-run full GC preview\n        if keep_date is None and dry_run:\n            logger.info(\"--- DRY RUN --- Would run 'nix-store --gc' (delete all unreferenced store paths). ---\")\n            if verbose:\n                logger.info(\"(Tip: for an approximate preview, run 'nix-collect-garbage -n' from a shell.)\")\n            return summary_info\n\n        # Full GC mode\n        if keep_date is None:\n            if not force:\n                proceed = _ask_yes_no(\"Run full Nix garbage collection (delete all unreferenced artifacts)?\", default=False)\n                if not proceed:\n                    logger.info(\"Operation cancelled.\")\n                    return summary_info\n            logger.info(\"Running Nix garbage collector...\")\n            try:\n                _, stdout, stderr = _safe_run([nix_bin, \"--gc\"], timeout=timeout_sec, check=True)\n                if stdout:\n                    if verbose:\n                        logger.info(stdout)\n                    else:\n                        rel = [l for l in stdout.splitlines() if re.search(r\"freed|removing|deleting\", l, re.I)]\n                        if rel:\n                            for line in rel[-10:]:\n                                logger.info(line)\n                logger.info(\"Garbage collection complete.\")\n                return summary_info\n            except RxpGCError as e:\n                raise\n\n        # Targeted deletion mode\n        if not logs_to_delete:\n            logger.info(\"No build logs older than %s found. Nothing to do.\", keep_date.isoformat())\n            return summary_info\n\n        if not delete_paths_all:\n            logger.info(\"No valid store paths found in logs older than %s. Nothing to delete.\", keep_date.isoformat())\n            return summary_info\n\n        prompt = f\"This will permanently delete {len(delete_paths_all)} store paths from {len(logs_to_delete)} build(s) older than {keep_date.isoformat()}. Continue?\"\n        if not force:\n            if not _ask_yes_no(prompt, default=False):\n                logger.info(\"Operation cancelled.\")\n                return summary_info\n\n        # Protect recent artifacts (date-based mode only) by adding indirect GC roots.\n        temp_gcroots_dir: Optional[Path] = None\n        protected = 0\n        try:\n            if keep_paths_all:\n                temp_gcroots_dir = Path(tempfile.mkdtemp(prefix=\"rixpress-gc-\"))\n                logger.info(\"Protecting %d recent artifacts via GC roots...\", len(keep_paths_all))\n                for i, p in enumerate(keep_paths_all, start=1):\n                    link_path = temp_gcroots_dir / f\"root-{i}\"\n                    try:\n                        # create a placeholder link path (the nix-store --add-root will create the gcroot)\n                        # use link_path as the path to register the indirect root\n                        _safe_run([nix_bin, \"--add-root\", str(link_path), \"--indirect\", p], timeout=timeout_sec, check=True)\n                        created_gcroot_links.append(link_path)\n                        protected += 1\n                    except RxpGCError as e:\n                        logger.warning(\"Failed to add GC root for %s: %s\", p, e)\n                if protected == 0:\n                    raise RxpGCError(\"Failed to protect any store paths. Aborting.\")\n                summary_info[\"protected\"] = protected\n\n            # Delete specific store paths\n            logger.info(\"Deleting %d targeted store paths...\", len(delete_paths_all))\n            existing_paths = [p for p in delete_paths_all if os.path.exists(p) or os.path.isdir(p)]\n            missing_paths = [p for p in delete_paths_all if p not in existing_paths]\n            if missing_paths:\n                logger.info(\"Skipping %d paths that no longer exist.\", len(missing_paths))\n                if verbose:\n                    for p in missing_paths:\n                        logger.info(\"  Missing: %s\", p)\n            if not existing_paths:\n                logger.info(\"No existing paths to delete. All targeted paths are already gone.\")\n                return summary_info\n\n            total_deleted = 0\n            failed_paths: List[str] = []\n            referenced_paths: List[str] = []\n\n            for i, pth in enumerate(existing_paths, start=1):\n                if not (os.path.exists(pth) or os.path.isdir(pth)):\n                    logger.info(\"  [%d/%d] Skipping %s (already gone)\", i, len(existing_paths), os.path.basename(pth))\n                    continue\n                logger.info(\"  [%d/%d] Attempting to delete %s...\", i, len(existing_paths), os.path.basename(pth))\n                try:\n                    proc = subprocess.run([nix_bin, \"--delete\", pth], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=timeout_sec)\n                    out = (proc.stdout or \"\") + \"\\n\" + (proc.stderr or \"\")\n                    if proc.returncode == 0:\n                        total_deleted += 1\n                        logger.info(\"    [OK] Successfully deleted\")\n                        if verbose and out.strip():\n                            logger.info(\"    %s\", out.strip())\n                    else:\n                        if re.search(r\"still alive|Cannot delete\", out, re.I):\n                            referenced_paths.append(pth)\n                            logger.info(\"    [!] Skipped (still referenced)\")\n                            if verbose:\n                                logger.info(\"    Details: %s\", out.strip())\n                        else:\n                            failed_paths.append(pth)\n                            logger.info(\"    [X] Failed to delete\")\n                            if verbose:\n                                logger.info(\"    Details: %s\", out.strip())\n                except subprocess.TimeoutExpired:\n                    failed_paths.append(pth)\n                    logger.info(\"    [X] Timeout while deleting\")\n                except Exception as e:\n                    failed_paths.append(pth)\n                    logger.info(\"    [X] Error: %s\", e)\n\n            # Summary of deletion\n            logger.info(\"\\nDeletion summary:\")\n            logger.info(\"  Successfully deleted: %d paths\", total_deleted)\n            logger.info(\"  Skipped (still referenced): %d paths\", len(referenced_paths))\n            logger.info(\"  Failed (other errors): %d paths\", len(failed_paths))\n\n            if referenced_paths and verbose:\n                logger.info(\"\\nReferenced paths (cannot delete):\")\n                for pth in referenced_paths:\n                    logger.info(\"  %s\", os.path.basename(pth))\n                    try:\n                        _, roots_out, _ = _safe_run([nix_bin, \"--query\", \"--roots\", pth], timeout=timeout_sec, check=False)\n                        if roots_out.strip():\n                            logger.info(\"    GC roots: %s\", roots_out.strip().replace(\"\\n\", \", \"))\n                        else:\n                            logger.info(\"    GC roots: (none found)\")\n                    except Exception:\n                        logger.info(\"    GC roots: (query failed)\")\n                    try:\n                        _, refs_out, _ = _safe_run([nix_bin, \"--query\", \"--referrers\", pth], timeout=timeout_sec, check=False)\n                        if refs_out.strip():\n                            refs = [os.path.basename(x) for x in refs_out.splitlines() if x.strip()]\n                            logger.info(\"    Referenced by: %s\", \", \".join(refs) if refs else \"(none)\")\n                        else:\n                            logger.info(\"    Referenced by: (none)\")\n                    except Exception:\n                        logger.info(\"    Referenced by: (query failed)\")\n\n            summary_info[\"deleted_count\"] = total_deleted\n            summary_info[\"failed_count\"] = len(failed_paths)\n            summary_info[\"referenced_count\"] = len(referenced_paths)\n\n            # Delete old build log files\n            if logs_to_delete:\n                logger.info(\"\\nDeleting old build log files...\")\n                log_files_deleted = 0\n                log_files_failed: List[str] = []\n                for i, entry in enumerate(logs_to_delete, start=1):\n                    log_file = entry[\"filename\"]\n                    log_path = project_path / \"_rixpress\" / log_file\n                    logger.info(\"  [%d/%d] Deleting %s...\", i, len(logs_to_delete), log_file)\n                    if not log_path.exists():\n                        logger.info(\"    [!] File not found (already deleted?)\")\n                        continue\n                    try:\n                        log_path.unlink()\n                        if not log_path.exists():\n                            log_files_deleted += 1\n                            logger.info(\"    [OK] Successfully deleted\")\n                        else:\n                            log_files_failed.append(log_file)\n                            logger.info(\"    [X] Failed to delete (file still exists)\")\n                    except Exception as e:\n                        log_files_failed.append(log_file)\n                        logger.info(\"    [X] Error: %s\", e)\n                logger.info(\"\\nBuild log deletion summary:\")\n                logger.info(\"  Successfully deleted: %d files\", log_files_deleted)\n                logger.info(\"  Failed: %d files\", len(log_files_failed))\n                if log_files_failed and verbose:\n                    logger.info(\"\\nFailed to delete log files:\")\n                    for lf in log_files_failed:\n                        logger.info(\"  %s\", lf)\n                summary_info[\"log_files_deleted\"] = log_files_deleted\n                summary_info[\"log_files_failed\"] = len(log_files_failed)\n\n            logger.info(\"\\nCleanup complete!\")\n            return summary_info\n        finally:\n            # Always attempt to remove created gcroot links and the temp dir\n            if created_gcroot_links:\n                for p in created_gcroot_links:\n                    try:\n                        if p.exists():\n                            p.unlink()\n                    except Exception:\n                        logger.debug(\"Failed to unlink gcroot link %s\", p)\n                # attempt to remove the parent temp directory if exists and empty\n                if temp_gcroots_dir and temp_gcroots_dir.exists():\n                    try:\n                        shutil.rmtree(temp_gcroots_dir)\n                    except Exception:\n                        # ignore: best-effort cleanup\n                        logger.debug(\"Failed to remove temp gcroots dir %s\", temp_gcroots_dir)\n    finally:\n        # always release lock and restore signals\n        try:\n            if lock_path_context is not None:\n                lock_path_context.release()\n        except Exception:\n            pass\n        try:\n            signal.signal(signal.SIGINT, old_sigint)\n            signal.signal(signal.SIGTERM, old_sigterm)\n        except Exception:\n            pass\n</code></pre>"},{"location":"reference/#ryxpress.rxp_init.rxp_init","title":"<code>rxp_init(project_path='.', skip_prompt=False)</code>","text":"<p>Initialize rixpress project files in project_path.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if initialization completed (or was skipped due to non-interactive but files present),</p> <code>bool</code> <p>False if cancelled by the user.</p> Source code in <code>src/ryxpress/rxp_init.py</code> <pre><code>def rxp_init(project_path: str = \".\", skip_prompt: bool = False) -&gt; bool:\n    \"\"\"\n    Initialize rixpress project files in project_path.\n\n    Returns:\n      True if initialization completed (or was skipped due to non-interactive but files present),\n      False if cancelled by the user.\n    \"\"\"\n    # Initial confirmation before any action\n    if not _confirm(f\"Initialize project at '{project_path}'?\", skip_prompt=skip_prompt):\n        print(\"Operation cancelled by user. No files or directories were created.\")\n        return False\n\n    proj = Path(project_path)\n    # Ensure project_path exists, create it if it doesn't\n    if not proj.exists():\n        proj.mkdir(parents=True, exist_ok=True)\n\n    env_file = proj / \"gen-env.R\"\n    pipeline_file = proj / \"gen-pipeline.R\"\n\n    gen_env_lines = [\n        \"# This script defines the default environment the pipeline runs in.\",\n        \"# Add the required packages to execute the code necessary for each derivation.\",\n        \"# If you want to create visual representations of the pipeline, consider adding\",\n        \"# `{visNetwork}` and `{ggdag}` to the list of R packages.\",\n        \"library(rix)\",\n        \"\",\n        \"# Define execution environment\",\n        \"rix(\",\n        \"  date = NULL,\",\n        \"  r_pkgs = NULL,\",\n        \"  py_conf = NULL,\",\n        \"  git_pkgs = list(\",\n        \"    \\\"package_name\\\" = \\\"rixpress\\\",\",\n        \"    \\\"repo_url\\\" = \\\"https://github.com/b-rodrigues/rixpress\\\",\",\n        \"    \\\"commit\\\" = \\\"HEAD\\\",\",\n        \"  ),\",\n        \"  ide = \\\"none\\\",\",\n        \"  project_path = \\\".\\\"\",\n        \")\",\n    ]\n\n    gen_pipeline_lines = [\n        \"library(rixpress)\",\n        \"library(igraph)\",\n        \"\",\n        \"list(\",\n        \"  rxp_r_file(\",\n        \"    name = NULL,\",\n        \"    path = NULL,\",\n        \"    read_function = \\\"lambda x: polars.read_csv(x, separator='|')\\\"\",\n        \"  ),\",\n        \"  rxp_r(\",\n        \"    name = NULL,\",\n        \"    expr = NULL\",\n        \"  )\",\n        \") |&gt;\",\n        \"  rxp_populate(build = FALSE)\",\n    ]\n\n    # Write files (overwrite if present)\n    env_file.write_text(\"\\n\".join(gen_env_lines) + \"\\n\", encoding=\"utf-8\")\n    print(f\"File {env_file} has been written.\")\n    pipeline_file.write_text(\"\\n\".join(gen_pipeline_lines) + \"\\n\", encoding=\"utf-8\")\n    print(f\"File {pipeline_file} has been written.\")\n\n    # Skip Git initialization when on non-interactive sessions (CRAN/CI/test equivalent)\n    if not _is_interactive():\n        print(\n            \"Skipping Git initialization (non-interactive session, CRAN, CI, or test environment detected).\"\n        )\n        return True\n\n    # Ask whether to initialise git\n    if _confirm(\"Would you like to initialise a Git repository here?\", skip_prompt=skip_prompt):\n        git_bin = shutil.which(\"git\")\n        if git_bin is None:\n            print(\n                \"Git not found on PATH. Please install git and run 'git init' manually, \"\n                \"or initialise the repository using your preferred tool.\"\n            )\n        else:\n            try:\n                subprocess.run([git_bin, \"init\"], cwd=str(proj), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                print(\"Git repository initialised.\")\n            except subprocess.CalledProcessError as e:\n                print(\"Failed to initialise git repository. You can run 'git init' manually.\")\n    else:\n        print(\"Skipping Git initialization.\")\n\n    return True\n</code></pre>"},{"location":"reference/#ryxpress.rxp_inspect.rxp_inspect","title":"<code>rxp_inspect(project_path='.', which_log=None)</code>","text":"<p>Inspect the build result of a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>project_path</code> <p>path to project root (defaults to \".\")</p> required <code>-</code> <code>which_log</code> <p>optional regex to select a specific log file. If None, the most recent log is used.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dict rows parsed from the selected JSON log file.</p> Source code in <code>src/ryxpress/rxp_inspect.py</code> <pre><code>def rxp_inspect(project_path: Union[str, Path] = \".\", which_log: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Inspect the build result of a pipeline.\n\n    Parameters:\n      - project_path: path to project root (defaults to \".\")\n      - which_log: optional regex to select a specific log file. If None, the most recent log is used.\n\n    Returns:\n      A list of dict rows parsed from the selected JSON log file.\n\n    Raises:\n      FileNotFoundError if no logs are found or _rixpress missing.\n      ValueError if which_log is provided but no matching filename is found.\n      RuntimeError if the chosen log cannot be read/parsed.\n    \"\"\"\n    proj = Path(project_path)\n    rixpress_dir = proj / \"_rixpress\"\n\n    # rxp_list_logs raises FileNotFoundError if no logs or dir missing\n    logs = rxp_list_logs(proj)\n\n    chosen_path: Optional[Path] = None\n\n    if which_log is None:\n        chosen_path = rixpress_dir / logs[0][\"filename\"]\n    else:\n        pattern = re.compile(which_log)\n        # Find first matching filename in logs (logs already sorted most-recent-first)\n        for entry in logs:\n            if pattern.search(entry[\"filename\"]):\n                chosen_path = rixpress_dir / entry[\"filename\"]\n                logger.info(\"Using log file: %s\", entry[\"filename\"])\n                break\n        if chosen_path is None:\n            raise ValueError(f\"No build logs found matching the pattern: {which_log}\")\n\n    try:\n        with chosen_path.open(\"r\", encoding=\"utf-8\") as fh:\n            data = json.load(fh)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to read log file {chosen_path}: {e}\")\n\n    rows = _coerce_json_to_rows(data)\n    return rows\n</code></pre>"},{"location":"reference/#ryxpress.rxp_inspect.rxp_list_logs","title":"<code>rxp_list_logs(project_path='.')</code>","text":"<p>List build logs in the project's _rixpress directory.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Union[str, float]]]</code> <p>A list of dictionaries, each with keys: - filename: basename of log file (str) - modification_time: ISO date string YYYY-MM-DD (str) - size_kb: file size in kilobytes rounded to 2 decimals (float)</p> Source code in <code>src/ryxpress/rxp_inspect.py</code> <pre><code>def rxp_list_logs(project_path: Union[str, Path] = \".\") -&gt; List[Dict[str, Union[str, float]]]:\n    \"\"\"\n    List build logs in the project's _rixpress directory.\n\n    Returns:\n      A list of dictionaries, each with keys:\n        - filename: basename of log file (str)\n        - modification_time: ISO date string YYYY-MM-DD (str)\n        - size_kb: file size in kilobytes rounded to 2 decimals (float)\n\n    Raises:\n      FileNotFoundError if the _rixpress directory does not exist or if no logs are found.\n    \"\"\"\n    proj = Path(project_path)\n    rixpress_dir = proj / \"_rixpress\"\n\n    if not rixpress_dir.exists() or not rixpress_dir.is_dir():\n        raise FileNotFoundError(\"_rixpress directory not found. Did you initialise the project?\")\n\n    pattern = re.compile(r\"^build_log.*\\.json$\")\n    log_files = [p for p in rixpress_dir.iterdir() if p.is_file() and pattern.search(p.name)]\n\n    # Sort by modification time (most recent first)\n    log_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n\n    if not log_files:\n        raise FileNotFoundError(f\"No build logs found in {rixpress_dir}\")\n\n    logs: List[Dict[str, Union[str, float]]] = []\n    for p in log_files:\n        st = p.stat()\n        logs.append(\n            {\n                \"filename\": p.name,\n                \"modification_time\": _iso_date_from_epoch(st.st_mtime),\n                \"size_kb\": round(st.st_size / 1024.0, 2),\n            }\n        )\n\n    return logs\n</code></pre>"},{"location":"reference/#ryxpress.rxp_read_load.rxp_load","title":"<code>rxp_load(derivation_name, which_log=None, project_path='.')</code>","text":"<p>Load the output of a derivation into the caller's globals under the name <code>derivation_name</code> if successfully loaded as an object.</p> <p>Otherwise return the path(s) (string or list[str]). Silent on failures.</p> Source code in <code>src/ryxpress/rxp_read_load.py</code> <pre><code>def rxp_load(\n    derivation_name: str,\n    which_log: Optional[str] = None,\n    project_path: Union[str, Path] = \".\",\n) -&gt; Union[object, str, List[str]]:\n    \"\"\"\n    Load the output of a derivation into the caller's globals under the name\n    `derivation_name` if successfully loaded as an object.\n\n    Otherwise return the path(s) (string or list[str]). Silent on failures.\n    \"\"\"\n    resolved = rxp_read_load_setup(derivation_name, which_log=which_log, project_path=project_path)\n\n    # If multiple outputs, return them\n    if isinstance(resolved, list):\n        return resolved\n\n    path = str(resolved)\n\n    if os.path.isdir(path):\n        return path\n\n    # Try to unpickle first\n    try:\n        with open(path, \"rb\") as fh:\n            obj = pickle.load(fh)\n    except Exception:\n        obj = None\n        logger.debug(\"pickle load failed for %s; will try rds2py if available\", path, exc_info=True)\n\n    # If pickle failed, try rds2py\n    if obj is None:\n        obj = _load_rds_with_rds2py(path)\n\n    if obj is None:\n        # Nothing we can load silently; return the path\n        return path\n\n    # Assign into caller's globals (best-effort); silence any assignment errors\n    try:\n        caller_frame = inspect.currentframe().f_back\n        if caller_frame is not None:\n            caller_globals = caller_frame.f_globals\n            # Use derivation_name as the variable name; keep last path component if it's a path\n            try:\n                var_name = derivation_name\n                # If derivation_name looks like a path, use the basename without extension\n                if derivation_name.startswith(\"/nix/store/\") or os.path.sep in derivation_name:\n                    var_name = os.path.splitext(os.path.basename(str(path)))[0]\n                # ensure valid identifier fallback\n                if not var_name.isidentifier():\n                    var_name = \"_\".join(re.findall(r\"\\w+\", var_name)) or \"loaded_artifact\"\n            except Exception:\n                var_name = \"loaded_artifact\"\n            caller_globals[var_name] = obj\n    except Exception:\n        logger.debug(\"Failed to assign loaded object into caller globals\", exc_info=True)\n\n    return obj\n</code></pre>"},{"location":"reference/#ryxpress.rxp_read_load.rxp_read","title":"<code>rxp_read(derivation_name, which_log=None, project_path='.')</code>","text":"<p>Read the output of a derivation.</p> <p>Behavior: - If resolved to multiple paths -&gt; return list[str]. - If single path:     1. If path is a directory -&gt; return the path string.     2. Try to pickle.load the file (regardless of extension). If successful, return object.     3. Try rds2py (if available) to parse; if successful, return object.     4. Otherwise return the path string. All failures are silent; no exceptions/warnings are raised for \"can't load\" cases.</p> Source code in <code>src/ryxpress/rxp_read_load.py</code> <pre><code>def rxp_read(\n    derivation_name: str,\n    which_log: Optional[str] = None,\n    project_path: Union[str, Path] = \".\",\n) -&gt; Union[object, str, List[str]]:\n    \"\"\"\n    Read the output of a derivation.\n\n    Behavior:\n    - If resolved to multiple paths -&gt; return list[str].\n    - If single path:\n        1. If path is a directory -&gt; return the path string.\n        2. Try to pickle.load the file (regardless of extension). If successful, return object.\n        3. Try rds2py (if available) to parse; if successful, return object.\n        4. Otherwise return the path string.\n    All failures are silent; no exceptions/warnings are raised for \"can't load\" cases.\n    \"\"\"\n    resolved = rxp_read_load_setup(derivation_name, which_log=which_log, project_path=project_path)\n\n    # If multiple outputs (list), return them directly\n    if isinstance(resolved, list):\n        return resolved\n\n    # Single path (string) or fallback value (derivation_name)\n    path = str(resolved)\n\n    # If path points to a directory, return it\n    if os.path.isdir(path):\n        return path\n\n    # Try to unpickle first (regardless of extension)\n    try:\n        with open(path, \"rb\") as fh:\n            obj = pickle.load(fh)\n        return obj\n    except Exception:\n        # Silent failure \u2014 try the next loader\n        logger.debug(\"pickle load failed for %s; will try rds2py if available\", path, exc_info=True)\n\n    # Try rds2py as a fallback (regardless of extension)\n    rds_obj = _load_rds_with_rds2py(path)\n    if rds_obj is not None:\n        return rds_obj\n\n    # Nothing worked; return the path string (no errors/warnings)\n    return path\n</code></pre>"},{"location":"reference/#ryxpress.rxp_read_load.rxp_read_load_setup","title":"<code>rxp_read_load_setup(derivation_name, which_log=None, project_path='.')</code>","text":"<p>Resolve derivation outputs.</p> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <ul> <li>single path as str if only one output resolved,</li> </ul> <code>Union[str, List[str]]</code> <ul> <li>list[str] if multiple outputs resolved,</li> </ul> <code>Union[str, List[str]]</code> <ul> <li>otherwise returns the original derivation_name (no exceptions raised here).</li> </ul> Source code in <code>src/ryxpress/rxp_read_load.py</code> <pre><code>def rxp_read_load_setup(\n    derivation_name: str,\n    which_log: Optional[str] = None,\n    project_path: Union[str, Path] = \".\",\n) -&gt; Union[str, List[str]]:\n    \"\"\"\n    Resolve derivation outputs.\n\n    Returns:\n      - single path as str if only one output resolved,\n      - list[str] if multiple outputs resolved,\n      - otherwise returns the original derivation_name (no exceptions raised here).\n    \"\"\"\n    # If given an explicit /nix/store path, handle directly\n    if isinstance(derivation_name, str) and derivation_name.startswith(\"/nix/store/\"):\n        store_path = Path(derivation_name)\n        try:\n            if store_path.is_dir():\n                files = [str(p) for p in sorted(store_path.iterdir())]\n                if len(files) == 1:\n                    return files[0]\n                else:\n                    # Mirror R behaviour: return the directory path string if multiple files\n                    return derivation_name\n            else:\n                # It's a file path -&gt; return it\n                return str(store_path)\n        except Exception:\n            # On any filesystem error, fall back to returning the original string\n            return derivation_name\n\n    # Otherwise, attempt to inspect build log; but do not raise on failure.\n    try:\n        rows = rxp_inspect(project_path=project_path, which_log=which_log)\n    except Exception:\n        # If inspection fails for any reason, return the original derivation_name\n        return derivation_name\n\n    if not isinstance(rows, list):\n        return derivation_name\n\n    # Find rows where the derivation column equals derivation_name.\n    deriv_keys = (\"derivation\", \"deriv\", \"name\")\n    path_key = \"path\"\n    output_key = \"output\"\n\n    matching_rows = []\n    for r in rows:\n        if not isinstance(r, dict):\n            continue\n        deriv_val = None\n        for k in deriv_keys:\n            if k in r:\n                deriv_val = r[k]\n                break\n        if deriv_val is None:\n            continue\n        if isinstance(deriv_val, (list, tuple)):\n            names = [str(x) for x in deriv_val if x is not None]\n        else:\n            names = [str(deriv_val)]\n        if derivation_name in names:\n            matching_rows.append(r)\n\n    if not matching_rows:\n        # Do not raise; return the original caller-supplied name\n        return derivation_name\n\n    # Collect outputs\n    file_paths: List[str] = []\n    for r in matching_rows:\n        base = r.get(path_key) or r.get(\"store_path\") or r.get(\"path_store\") or r.get(\"output_path\")\n        if base is None:\n            continue\n        base_str = str(base)\n        outs = r.get(output_key)\n        if outs is None:\n            file_paths.append(base_str)\n            continue\n        if isinstance(outs, (list, tuple)):\n            out_list = [str(x) for x in outs if x is not None]\n        else:\n            out_list = [str(outs)]\n        for o in out_list:\n            if str(o).startswith(\"/\"):\n                file_paths.append(o)\n            else:\n                file_paths.append(os.path.join(base_str, o))\n\n    # Deduplicate while preserving order\n    seen = set()\n    deduped: List[str] = []\n    for p in file_paths:\n        if p not in seen:\n            seen.add(p)\n            deduped.append(p)\n\n    if len(deduped) == 0:\n        # No outputs found; return original derivation_name instead of raising\n        return derivation_name\n\n    if len(deduped) == 1:\n        return deduped[0]\n    return deduped\n</code></pre>"},{"location":"reference/#ryxpress.plot_dag.get_nodes_edges","title":"<code>get_nodes_edges(path_dag='_rixpress/dag.json')</code>","text":"<p>Read _rixpress/dag.json and return a dict with 'nodes' and 'edges'.</p> <p>nodes: list of {\"id\": , \"label\": , \"group\": } edges: list of {\"from\": , \"to\": , \"arrows\": \"to\"} <p>Raises FileNotFoundError if the JSON file is missing, ValueError if the JSON contents don't contain derivations.</p> Source code in <code>src/ryxpress/plot_dag.py</code> <pre><code>def get_nodes_edges(path_dag: Union[str, Path] = \"_rixpress/dag.json\") -&gt; Dict[str, List[Dict]]:\n    \"\"\"\n    Read _rixpress/dag.json and return a dict with 'nodes' and 'edges'.\n\n    nodes: list of {\"id\": &lt;name&gt;, \"label\": &lt;name&gt;, \"group\": &lt;type&gt;}\n    edges: list of {\"from\": &lt;dep&gt;, \"to\": &lt;deriv&gt;, \"arrows\": \"to\"}\n\n    Raises FileNotFoundError if the JSON file is missing, ValueError if the\n    JSON contents don't contain derivations.\n    \"\"\"\n    path = Path(path_dag)\n    if not path.exists():\n        raise FileNotFoundError(\"dag.json missing! Did you run 'rxp_populate()'?\")\n\n    data = json.loads(path.read_text(encoding=\"utf-8\"))\n    derivations = data.get(\"derivations\")\n    if derivations is None:\n        raise ValueError(\"No 'derivations' key found in dag.json\")\n\n    nodes_seen = {}\n    edges: List[Dict[str, str]] = []\n\n    # derivations is expected to be a list of dict-like objects\n    for entry in derivations:\n        if not isinstance(entry, dict):\n            continue\n        # Derivation name: prefer 'deriv_name', fall back to 'name' or 'derivation'\n        deriv_name = entry.get(\"deriv_name\") or entry.get(\"name\") or entry.get(\"derivation\")\n        if deriv_name is None:\n            continue\n        # type (group)\n        group = None\n        t = entry.get(\"type\")\n        if t is not None:\n            # type might be scalar or list; normalize to first value if list\n            t_list = _normalize_to_list(t)\n            group = t_list[0] if t_list else None\n\n        # Add node (deduplicated)\n        id_str = str(deriv_name)\n        if id_str not in nodes_seen:\n            nodes_seen[id_str] = {\"id\": id_str, \"label\": id_str, \"group\": group}\n\n        # dependencies: could be absent, scalar, or list\n        depends = entry.get(\"depends\")\n        dep_list = _normalize_to_list(depends)\n        for dep in dep_list:\n            dep_str = str(dep)\n            edges.append({\"from\": dep_str, \"to\": id_str, \"arrows\": \"to\"})\n            # Note: we do NOT automatically create nodes for dependencies that are\n            # not present as derivations in the file (mirrors R behavior).\n            # If you'd like dependency-only nodes included, uncomment below:\n            # if dep_str not in nodes_seen:\n            #     nodes_seen[dep_str] = {\"id\": dep_str, \"label\": dep_str, \"group\": None}\n\n    # Convert nodes_seen to a list preserving insertion order\n    nodes = list(nodes_seen.values())\n\n    return {\"nodes\": nodes, \"edges\": edges}\n</code></pre> <p>Trace lineage of derivations \u2014 translation of R's rxp_trace to Python.</p> <p>Behavior: - Reads a dag.json (default: _rixpress/dag.json) and expects a top-level object   with a \"derivations\" list. - Builds dependency (depends_map) and reverse-dependency maps. - If name is provided, prints a lineage for that derivation (ancestors and children). - If name is None, prints an inverted global pipeline view (outputs -&gt; inputs),   starting from sinks (nodes with no children). - transitive=True shows transitive closure and marks transitive-only nodes with \"*\". - include_self=True includes the node itself in returned dependency lists. - Returns a dict mapping each derivation name to {\"dependencies\": [...], \"reverse_dependencies\": [...]}. - Raises FileNotFoundError / ValueError / RuntimeError for missing/invalid inputs.</p>"},{"location":"reference/#ryxpress.plot_dag.rxp_dag_for_ci","title":"<code>rxp_dag_for_ci(nodes_and_edges=None, output_file='_rixpress/dag.dot')</code>","text":"<p>Build an igraph object from nodes_and_edges and write a DOT file for CI.</p> <ul> <li>nodes_and_edges: dict with keys 'nodes' and 'edges' as returned by   get_nodes_edges(). If None, get_nodes_edges() is called.</li> <li>output_file: path to write DOT file. Parent directories are created as needed.</li> </ul> <p>Raises ImportError if python-igraph is not installed.</p> Source code in <code>src/ryxpress/plot_dag.py</code> <pre><code>def rxp_dag_for_ci(nodes_and_edges: Optional[Dict[str, List[Dict]]] = None,\n                   output_file: Union[str, Path] = \"_rixpress/dag.dot\") -&gt; None:\n    \"\"\"\n    Build an igraph object from nodes_and_edges and write a DOT file for CI.\n\n    - nodes_and_edges: dict with keys 'nodes' and 'edges' as returned by\n      get_nodes_edges(). If None, get_nodes_edges() is called.\n    - output_file: path to write DOT file. Parent directories are created as needed.\n\n    Raises ImportError if python-igraph is not installed.\n    \"\"\"\n    # Lazy import igraph and raise helpful error if not available\n    try:\n        import igraph  # python-igraph\n    except Exception as e:  # ImportError or other import-time errors\n        raise ImportError(\n            \"The python 'igraph' package is required for rxp_dag_for_ci. \"\n            \"Install it with e.g. 'pip install python-igraph' and try again.\"\n        ) from e\n\n    if nodes_and_edges is None:\n        nodes_and_edges = get_nodes_edges()\n\n    edges = nodes_and_edges.get(\"edges\", [])\n    # Build a list of tuples (from, to) for igraph\n    edge_tuples = [(e[\"from\"], e[\"to\"]) for e in edges]\n\n    # Ensure output directory exists\n    out_path = Path(output_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Create the graph from edge tuples. TupleList will create vertices named by\n    # the unique labels encountered in the tuples.\n    # If there are no edges but there are nodes, create an empty graph and add vertices.\n    if edge_tuples:\n        g = igraph.Graph.TupleList(edge_tuples, directed=True, vertex_name_attr=\"name\")\n    else:\n        # no edges \u2014 create graph and add vertices from nodes list\n        nodes = nodes_and_edges.get(\"nodes\", [])\n        vertex_names = [n[\"id\"] for n in nodes]\n        g = igraph.Graph(directed=True)\n        if vertex_names:\n            g.add_vertices(vertex_names)\n            # set the 'name' attribute automatically when vertices are named\n\n    # Set vertex 'label' attribute from vertex name\n    # g.vs['name'] should exist; copy to 'label'\n    try:\n        names = g.vs[\"name\"]\n        g.vs[\"label\"] = names\n        # Attempt to remove the 'name' attribute to mirror R behavior.\n        # python-igraph allows deleting vertex attributes via 'del g.vs[\"attr\"]'.\n        try:\n            del g.vs[\"name\"]\n        except Exception:\n            # If deletion is not supported in some igraph versions, leave it;\n            # having both 'name' and 'label' is harmless for DOT output.\n            logger.debug(\"Could not delete 'name' vertex attribute; leaving it in place.\")\n    except Exception:\n        # If the graph has no vertices or attribute access fails, continue.\n        pass\n\n    # Write graph to DOT format\n    # Use Graph.write with format=\"dot\"\n    try:\n        g.write(str(out_path), format=\"dot\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to write DOT file to {out_path}: {e}\") from e\n</code></pre>"},{"location":"reference/#ryxpress.rxp_trace.rxp_trace","title":"<code>rxp_trace(name=None, dag_file=Path('_rixpress') / 'dag.json', transitive=True, include_self=False)</code>","text":"<p>Trace lineage of derivations.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, List[str]]]</code> <p>A dict mapping each inspected derivation name to a dict with keys: - 'dependencies' : list of dependency names (ancestors), with transitive-only names marked with '' - 'reverse_dependencies' : list of reverse dependents (children), with transitive-only names marked with ''</p> Side-effect <p>Prints a tree representation to stdout (either the whole pipeline or the single-node lineage).</p> Source code in <code>src/ryxpress/rxp_trace.py</code> <pre><code>def rxp_trace(\n    name: Optional[str] = None,\n    dag_file: Union[str, Path] = Path(\"_rixpress\") / \"dag.json\",\n    transitive: bool = True,\n    include_self: bool = False,\n) -&gt; Dict[str, Dict[str, List[str]]]:\n    \"\"\"\n    Trace lineage of derivations.\n\n    Returns:\n      A dict mapping each inspected derivation name to a dict with keys:\n        - 'dependencies' : list of dependency names (ancestors), with transitive-only names marked with '*'\n        - 'reverse_dependencies' : list of reverse dependents (children), with transitive-only names marked with '*'\n\n    Side-effect:\n      Prints a tree representation to stdout (either the whole pipeline or\n      the single-node lineage).\n    \"\"\"\n    derivs = _load_dag(dag_file)\n\n    all_names: List[str] = []\n    for d in derivs:\n        nm = _extract_name(d)\n        if nm is None:\n            raise ValueError(\"Found derivations with missing or unparsable names in dag.json.\")\n        all_names.append(nm)\n\n    if name is not None and name not in all_names:\n        # mirror R's head(...) behaviour for listing available names\n        snippet = \", \".join(all_names[:20])\n        more = \", ...\" if len(all_names) &gt; 20 else \"\"\n        raise ValueError(f\"Derivation '{name}' not found in dag.json (available: {snippet}{more}).\")\n\n    depends_map = _make_depends_map(derivs, all_names)\n    reverse_map = _build_reverse_map(depends_map, all_names)\n\n    # helper to print single lineage (deps and reverse deps)\n    def print_single(target: str) -&gt; None:\n        print(f\"==== Lineage for: {target} ====\")\n        # Dependencies (ancestors)\n        print(\"Dependencies (ancestors):\")\n        visited: List[str] = []\n\n        def rec_dep(node: str, depth: int) -&gt; None:\n            parents = depends_map.get(node) or []\n            if not parents:\n                if depth == 0:\n                    print(\"  - &lt;none&gt;\")\n                return\n            for p in parents:\n                label = f\"{p}*\" if (transitive and depth &gt;= 1) else p\n                print((\"  \" * (depth + 1)) + \"- \" + label)\n                if p not in visited:\n                    visited.append(p)\n                    rec_dep(p, depth + 1)\n\n        rec_dep(target, 0)\n\n        print(\"\\nReverse dependencies (children):\")\n        visited = []\n\n        def rec_rev(node: str, depth: int) -&gt; None:\n            kids = reverse_map.get(node) or []\n            if not kids:\n                if depth == 0:\n                    print(\"  - &lt;none&gt;\")\n                return\n            for k in kids:\n                label = f\"{k}*\" if (transitive and depth &gt;= 1) else k\n                print((\"  \" * (depth + 1)) + \"- \" + label)\n                if k not in visited:\n                    visited.append(k)\n                    rec_rev(k, depth + 1)\n\n        rec_rev(target, 0)\n\n        if transitive:\n            print(\"\\nNote: '*' marks transitive dependencies (depth &gt;= 2).\\n\")\n\n    # helper to print forest starting from given roots, using depends_map (outputs -&gt; inputs)\n    def print_forest_once(roots: List[str], graph: Dict[str, List[str]], transitive_flag: bool) -&gt; None:\n        visited_nodes: List[str] = []\n\n        def rec(node: str, depth: int) -&gt; None:\n            label = f\"{node}*\" if (transitive_flag and depth &gt;= 2) else node\n            print((\"  \" * depth) + \"- \" + label)\n            if node in visited_nodes:\n                return\n            visited_nodes.append(node)\n            kids = graph.get(node) or []\n            if not kids:\n                return\n            for k in kids:\n                rec(k, depth + 1)\n\n        for r in roots:\n            rec(r, 0)\n\n    # sinks: nodes with no children in reverse_map\n    def sinks() -&gt; List[str]:\n        no_children = [n for n, kids in reverse_map.items() if not kids]\n        if no_children:\n            return no_children\n        outdeg_vals = {n: len(kids) for n, kids in reverse_map.items()}\n        if outdeg_vals:\n            min_outdeg = min(outdeg_vals.values())\n            return [n for n, v in outdeg_vals.items() if v == min_outdeg]\n        return []\n\n    # Build results mapping\n    results: Dict[str, Dict[str, List[str]]] = {}\n    for nm in all_names:\n        deps = _marked_vec(nm, depends_map, transitive)\n        rdeps = _marked_vec(nm, reverse_map, transitive)\n        if include_self:\n            deps = _unique_preserve_order([nm] + deps)\n            rdeps = _unique_preserve_order([nm] + rdeps)\n        results[nm] = {\"dependencies\": deps, \"reverse_dependencies\": rdeps}\n\n    if name is None:\n        print(\"==== Pipeline dependency tree (outputs \\u2192 inputs) ====\")\n        for root in sinks():\n            print_forest_once([root], depends_map, transitive)\n        if transitive:\n            print(\"\\nNote: '*' marks transitive dependencies (depth &gt;= 2).\\n\")\n        return results\n    else:\n        print_single(name)\n        # return only the single-name mapping to match the R invisible(results[name]) behaviour\n        return {name: results[name]}\n</code></pre>"}]}