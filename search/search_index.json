{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ryxpress","text":"<p><code>ryxpress</code> is a Python reimplementation/port of the R package rixpress. It provides helpers and a small framework to build and work with reproducible, multilanguage analytical pipelines that are built with Nix. <code>ryxpress</code> requires an installation of R and <code>rixpress</code> to run, but this can be handled quite easily and transparently thanks to Nix.</p> <p>If you previously used the R version (<code>rixpress</code>), <code>ryxpress</code> aims to provide a similar user experience for Python projects while integrating with the same Nix-first workflow.</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>TBC</p> <p>This package is in early alpha and requires more work.</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#start-a-project-and-build-the-pipeline","title":"Start a project and build the pipeline","text":"<p>rxp_init</p> <p>Initialize rixpress project files in project_path. This will generate two R scripts: <code>gen-env.R</code>, which when executed using the rix R package will generate a <code>default.nix</code>, which defines the pipeline's execution environment, and <code>gen-pipeline.R</code>, which is where the pipeline is defined. These R scripts are the same as those generated by rixpress, the R version of this package.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if initialization completed (or was skipped due to non-interactive but files present),</p> <code>bool</code> <p>False if cancelled by the user.</p> Source code in <code>src/ryxpress/init_proj.py</code> <pre><code>def rxp_init(project_path: str = \".\", skip_prompt: bool = False) -&gt; bool:\n    \"\"\"\n    rxp_init\n\n    Initialize rixpress project files in project_path. This will generate\n    two R scripts: `gen-env.R`, which when executed using the rix R package\n    will generate a `default.nix`, which defines the pipeline's execution\n    environment, and `gen-pipeline.R`, which is where the pipeline is defined.\n    These R scripts are the same as those generated by rixpress, the R version\n    of this package.\n\n    Returns:\n      True if initialization completed (or was skipped due to non-interactive but files present),\n      False if cancelled by the user.\n    \"\"\"\n    # Initial confirmation before any action\n    if not _confirm(f\"Initialize project at '{project_path}'?\", skip_prompt=skip_prompt):\n        print(\"Operation cancelled by user. No files or directories were created.\")\n        return False\n\n    proj = Path(project_path)\n    # Ensure project_path exists, create it if it doesn't\n    if not proj.exists():\n        proj.mkdir(parents=True, exist_ok=True)\n\n    env_file = proj / \"gen-env.R\"\n    pipeline_file = proj / \"gen-pipeline.R\"\n\n    gen_env_lines = [\n        \"# This script defines the default environment the pipeline runs in.\",\n        \"# Add the required packages to execute the code necessary for each derivation.\",\n        \"# If you want to create visual representations of the pipeline, consider adding\",\n        \"# `{visNetwork}` and `{ggdag}` to the list of R packages.\",\n        \"library(rix)\",\n        \"\",\n        \"# Define execution environment\",\n        \"rix(\",\n        \"  date = NULL,\",\n        \"  r_pkgs = NULL,\",\n        \"  py_conf = NULL,\",\n        \"  git_pkgs = list(\",\n        \"    \\\"package_name\\\" = \\\"rixpress\\\",\",\n        \"    \\\"repo_url\\\" = \\\"https://github.com/b-rodrigues/rixpress\\\",\",\n        \"    \\\"commit\\\" = \\\"HEAD\\\",\",\n        \"  ),\",\n        \"  ide = \\\"none\\\",\",\n        \"  project_path = \\\".\\\"\",\n        \")\",\n    ]\n\n    gen_pipeline_lines = [\n        \"library(rixpress)\",\n        \"library(igraph)\",\n        \"\",\n        \"list(\",\n        \"  rxp_r_file(\",\n        \"    name = NULL,\",\n        \"    path = NULL,\",\n        \"    read_function = \\\"lambda x: polars.read_csv(x, separator='|')\\\"\",\n        \"  ),\",\n        \"  rxp_r(\",\n        \"    name = NULL,\",\n        \"    expr = NULL\",\n        \"  )\",\n        \") |&gt;\",\n        \"  rxp_populate(build = FALSE)\",\n    ]\n\n    # Write files (overwrite if present)\n    env_file.write_text(\"\\n\".join(gen_env_lines) + \"\\n\", encoding=\"utf-8\")\n    print(f\"File {env_file} has been written.\")\n    pipeline_file.write_text(\"\\n\".join(gen_pipeline_lines) + \"\\n\", encoding=\"utf-8\")\n    print(f\"File {pipeline_file} has been written.\")\n\n    # Skip Git initialization when on non-interactive sessions (CRAN/CI/test equivalent)\n    if not _is_interactive():\n        print(\n            \"Skipping Git initialization (non-interactive session, CRAN, CI, or test environment detected).\"\n        )\n        return True\n\n    # Ask whether to initialise git\n    if _confirm(\"Would you like to initialise a Git repository here?\", skip_prompt=skip_prompt):\n        git_bin = shutil.which(\"git\")\n        if git_bin is None:\n            print(\n                \"Git not found on PATH. Please install git and run 'git init' manually, \"\n                \"or initialise the repository using your preferred tool.\"\n            )\n        else:\n            try:\n                subprocess.run([git_bin, \"init\"], cwd=str(proj), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                print(\"Git repository initialised.\")\n            except subprocess.CalledProcessError as e:\n                print(\"Failed to initialise git repository. You can run 'git init' manually.\")\n    else:\n        print(\"Skipping Git initialization.\")\n\n    return True\n</code></pre> <p>rxp_make</p> <p>Run the rixpress R pipeline (rxp_populate + rxp_make) by sourcing an R script.</p> <p>Parameters:</p> <ul> <li>script: Path or name of the R script to run (defaults to \"gen-pipeline.R\").           If a relative path is given and doesn't exist in the working directory,           this function will attempt to locate the script on PATH.</li> <li>verbose: integer passed to rixpress::rxp_make(verbose = ...)</li> <li>max_jobs: integer passed to rixpress::rxp_make(max_jobs = ...)</li> <li>cores: integer passed to rixpress::rxp_make(cores = ...)</li> <li>rscript_cmd: the Rscript binary to use (defaults to \"Rscript\")</li> <li>timeout: optional timeout in seconds for the subprocess.run call</li> <li>cwd: optional working directory to run Rscript in. If None, the directory        containing the provided script will be used. This is important because        pipeline.nix and related files are often imported with relative paths        (e.g. ./default.nix), so Rscript needs to be run where those files are reachable.</li> </ul> <p>Returns an RRunResult containing returncode, stdout, stderr.</p> Source code in <code>src/ryxpress/r_runner.py</code> <pre><code>def rxp_make(\n    script: Union[str, Path] = \"gen-pipeline.R\",\n    verbose: int = 0,\n    max_jobs: int = 1,\n    cores: int = 1,\n    rscript_cmd: str = \"Rscript\",\n    timeout: Optional[int] = None,\n    cwd: Optional[Union[str, Path]] = None,\n) -&gt; RRunResult:\n    \"\"\"\n    rxp_make\n\n    Run the rixpress R pipeline (rxp_populate + rxp_make) by sourcing an R script.\n\n    Parameters:\n\n    - script: Path or name of the R script to run (defaults to \"gen-pipeline.R\").\n              If a relative path is given and doesn't exist in the working directory,\n              this function will attempt to locate the script on PATH.\n    - verbose: integer passed to rixpress::rxp_make(verbose = ...)\n    - max_jobs: integer passed to rixpress::rxp_make(max_jobs = ...)\n    - cores: integer passed to rixpress::rxp_make(cores = ...)\n    - rscript_cmd: the Rscript binary to use (defaults to \"Rscript\")\n    - timeout: optional timeout in seconds for the subprocess.run call\n    - cwd: optional working directory to run Rscript in. If None, the directory\n           containing the provided script will be used. This is important because\n           pipeline.nix and related files are often imported with relative paths\n           (e.g. ./default.nix), so Rscript needs to be run where those files are reachable.\n\n    Returns an RRunResult containing returncode, stdout, stderr.\n    \"\"\"\n    # Validate integers\n    for name, val in ((\"verbose\", verbose), (\"max_jobs\", max_jobs), (\"cores\", cores)):\n        if not isinstance(val, int):\n            raise TypeError(f\"{name} must be an int, got {type(val).__name__}\")\n        if val &lt; 0:\n            raise ValueError(f\"{name} must be &gt;= 0\")\n\n    # Resolve script path: prefer given path if it exists; otherwise try to find on PATH\n    script_path = Path(script)\n    if not script_path.is_file():\n        # If a bare name was provided, attempt to find it on PATH\n        found = shutil.which(str(script))\n        if found:\n            script_path = Path(found)\n        else:\n            raise FileNotFoundError(\n                f\"R script '{script}' not found in working directory and not on PATH\"\n            )\n    else:\n        script_path = script_path.resolve()\n\n    # Determine working directory for the R process:\n    if cwd is not None:\n        run_cwd = Path(cwd).resolve()\n        if not run_cwd.is_dir():\n            raise FileNotFoundError(f\"Requested cwd '{cwd}' does not exist or is not a directory\")\n    else:\n        # default to the script's parent directory so relative imports (./default.nix) work\n        run_cwd = script_path.parent\n\n    # Verify Rscript binary exists\n    if shutil.which(rscript_cmd) is None:\n        raise FileNotFoundError(\n            f\"Rscript binary '{rscript_cmd}' not found in PATH. Ensure R is installed or adjust rscript_cmd.\"\n        )\n\n    # Prepare wrapper R script that:\n    #  - loads rixpress,\n    #  - sources the user's script,\n    #  - if the sourced evaluation returns a list, calls rxp_populate on it,\n    #  - then calls rixpress::rxp_make(...) with the provided args.\n    wrapper = f\"\"\"\nsuppressPackageStartupMessages(library(rixpress))\n\nscript_path &lt;- \"{script_path.as_posix()}\"\n\nif (!file.exists(script_path)) {{\n  stop(\"Script not found: \", script_path)\n}}\n\nresult_value &lt;- NULL\n\nres &lt;- tryCatch({{\n  # Source &amp; evaluate the user's script and capture the returned value (if any)\n  result_value &lt;- eval(parse(script_path))\n  # If the script returned a list (a pipeline), run rxp_populate on it\n  if (!is.null(result_value) &amp;&amp; is.list(result_value)) {{\n    pipeline &lt;- result_value\n    pipeline &lt;- rixpress::rxp_populate(pipeline)\n  }}\n  # Finally, run rxp_make with the given integer parameters\n  rixpress::rxp_make(\n    verbose = {int(verbose)},\n    max_jobs = {int(max_jobs)},\n    cores = {int(cores)}\n  )\n}}, error = function(e) {{\n  # Print a clear error message and exit with non-zero status\n  message(\"rixpress-python-runner-error: \", conditionMessage(e))\n  quit(status = 1)\n}})\n\n# If we reach here, exit with success\nquit(status = 0)\n\"\"\"\n\n    # Create temporary file for wrapper\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".R\", delete=False) as tf:\n        tf.write(wrapper)\n        wrapper_path = Path(tf.name)\n\n    try:\n        # Run Rscript on the wrapper file using the desired working directory\n        proc = subprocess.run(\n            [rscript_cmd, str(wrapper_path)],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=timeout,\n            cwd=str(run_cwd),\n        )\n        return RRunResult(returncode=proc.returncode, stdout=proc.stdout, stderr=proc.stderr)\n    finally:\n        try:\n            wrapper_path.unlink()\n        except Exception:\n            pass\n</code></pre>"},{"location":"reference/#inspect-the-pipeline","title":"Inspect the pipeline","text":"<p>rxp_inspect</p> <p>Inspect the build result of a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>project_path</code> <p>path to project root (defaults to \".\")</p> required <code>-</code> <code>which_log</code> <p>optional regex to select a specific log file. If None, the most recent log is used.</p> required <code>-</code> <code>pretty</code> <p>if True, pretty-prints the result (and returns nothing).</p> required <code>-</code> <code>as_json</code> <p>if True, pretty prints using json.dumps(indent=2) instead of pprint.</p> required <p>Returns:</p> Type Description <code>Optional[List[Dict[str, Any]]]</code> <p>A list of dict rows parsed from the selected JSON log file (unless pretty=True).</p> Source code in <code>src/ryxpress/inspect_logs.py</code> <pre><code>def rxp_inspect(\n    project_path: Union[str, Path] = \".\",\n    which_log: Optional[str] = None,\n    pretty: bool = False,\n    as_json: bool = False,\n) -&gt; Optional[List[Dict[str, Any]]]:\n    \"\"\"\n    rxp_inspect\n\n    Inspect the build result of a pipeline.\n\n    Parameters:\n      - project_path: path to project root (defaults to \".\")\n      - which_log: optional regex to select a specific log file. If None, the most recent log is used.\n      - pretty: if True, pretty-prints the result (and returns nothing).\n      - as_json: if True, pretty prints using json.dumps(indent=2) instead of pprint.\n\n    Returns:\n      A list of dict rows parsed from the selected JSON log file (unless pretty=True).\n\n    Raises:\n      FileNotFoundError if no logs are found or _rixpress missing.\n      ValueError if which_log is provided but no matching filename is found.\n      RuntimeError if the chosen log cannot be read/parsed.\n    \"\"\"\n    proj = Path(project_path)\n    rixpress_dir = proj / \"_rixpress\"\n\n    logs = rxp_list_logs(proj)\n\n    chosen_path: Optional[Path] = None\n\n    if which_log is None:\n        chosen_path = rixpress_dir / logs[0][\"filename\"]\n    else:\n        import re, logging\n        logger = logging.getLogger(__name__)\n        pattern = re.compile(which_log)\n        for entry in logs:\n            if pattern.search(entry[\"filename\"]):\n                chosen_path = rixpress_dir / entry[\"filename\"]\n                logger.info(\"Using log file: %s\", entry[\"filename\"])\n                break\n        if chosen_path is None:\n            raise ValueError(f\"No build logs found matching the pattern: {which_log}\")\n\n    try:\n        with chosen_path.open(\"r\", encoding=\"utf-8\") as fh:\n            data = json.load(fh)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to read log file {chosen_path}: {e}\")\n\n    rows = _coerce_json_to_rows(data)\n\n    if pretty:\n        if as_json:\n            print(json.dumps(rows, indent=2, ensure_ascii=False))\n        else:\n            pprint(rows)\n        return  # This ensures REPL shows nothing after print, return value is None\n\n    return rows\n</code></pre> <p>rxp_list_logs</p> <p>List build logs in the project's _rixpress directory.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>project_path</code> <p>path to project root (defaults to \".\")</p> required <code>-</code> <code>pretty</code> <p>if True, pretty-prints the result (and returns nothing).</p> required <code>-</code> <code>as_json</code> <p>if True, pretty prints using json.dumps(indent=2) instead of pprint.</p> required <p>Returns:</p> Type Description <code>Optional[List[Dict[str, Union[str, float]]]]</code> <p>A list of dictionaries, each with keys: - filename: basename of log file (str) - modification_time: ISO date string YYYY-MM-DD (str) - size_kb: file size in kilobytes rounded to 2 decimals (float)</p> <code>Optional[List[Dict[str, Union[str, float]]]]</code> <p>(unless pretty=True, in which case nothing is returned)</p> Source code in <code>src/ryxpress/inspect_logs.py</code> <pre><code>def rxp_list_logs(\n    project_path: Union[str, Path] = \".\",\n    pretty: bool = False,\n    as_json: bool = False,\n) -&gt; Optional[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    rxp_list_logs\n\n    List build logs in the project's _rixpress directory.\n\n    Parameters:\n      - project_path: path to project root (defaults to \".\")\n      - pretty: if True, pretty-prints the result (and returns nothing).\n      - as_json: if True, pretty prints using json.dumps(indent=2) instead of pprint.\n\n    Returns:\n      A list of dictionaries, each with keys:\n        - filename: basename of log file (str)\n        - modification_time: ISO date string YYYY-MM-DD (str)\n        - size_kb: file size in kilobytes rounded to 2 decimals (float)\n      (unless pretty=True, in which case nothing is returned)\n\n    Raises:\n      FileNotFoundError if the _rixpress directory does not exist or if no logs are found.\n    \"\"\"\n    proj = Path(project_path)\n    rixpress_dir = proj / \"_rixpress\"\n\n    if not rixpress_dir.exists() or not rixpress_dir.is_dir():\n        raise FileNotFoundError(\"_rixpress directory not found. Did you initialise the project?\")\n\n    pattern = re.compile(r\"^build_log.*\\.json$\")\n    log_files = [p for p in rixpress_dir.iterdir() if p.is_file() and pattern.search(p.name)]\n\n    # Sort by modification time (most recent first)\n    log_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n\n    if not log_files:\n        raise FileNotFoundError(f\"No build logs found in {rixpress_dir}\")\n\n    logs: List[Dict[str, Union[str, float]]] = []\n    for p in log_files:\n        st = p.stat()\n        logs.append(\n            {\n                \"filename\": p.name,\n                \"modification_time\": _iso_date_from_epoch(st.st_mtime),\n                \"size_kb\": round(st.st_size / 1024.0, 2),\n            }\n        )\n\n    if pretty:\n        if as_json:\n            print(json.dumps(logs, indent=2, ensure_ascii=False))\n        else:\n            pprint(logs)\n        return\n\n    return logs\n</code></pre>"},{"location":"reference/#recover-artifacts","title":"Recover artifacts","text":"<p>rxp_copy</p> <p>Copy derivations from the Nix store to ./pipeline-output.</p> <p>Parameters:</p> <ul> <li>derivation_name: name of the derivation to copy (string). If None,     uses the special derivation name \"all-derivations\" (mirrors R).</li> <li>dir_mode / file_mode: octal permission strings applied to copied dirs/files.</li> <li>project_path: project root where _rixpress lives (defaults to \".\").</li> </ul> <p>Raises:</p> <ul> <li>FileNotFoundError if _rixpress or logs are missing.</li> <li>ValueError on invalid modes or derivation not found.</li> <li>RuntimeError on copy failures.</li> </ul> Source code in <code>src/ryxpress/copy_artifacts.py</code> <pre><code>def rxp_copy(\n    derivation_name: Optional[str] = None,\n    dir_mode: str = \"0755\",\n    file_mode: str = \"0644\",\n    project_path: Union[str, Path] = \".\",\n) -&gt; None:\n    \"\"\"\n    rxp_copy\n\n    Copy derivations from the Nix store to ./pipeline-output.\n\n    Parameters:\n\n      - derivation_name: name of the derivation to copy (string). If None,\n        uses the special derivation name \"all-derivations\" (mirrors R).\n      - dir_mode / file_mode: octal permission strings applied to copied dirs/files.\n      - project_path: project root where _rixpress lives (defaults to \".\").\n\n    Raises:\n\n      - FileNotFoundError if _rixpress or logs are missing.\n      - ValueError on invalid modes or derivation not found.\n      - RuntimeError on copy failures.\n    \"\"\"\n    project = Path(project_path)\n    # Validate modes\n    if not _valid_mode(dir_mode):\n        raise ValueError('Invalid dir_mode: provide a character octal like \"0755\" or \"755\".')\n    if not _valid_mode(file_mode):\n        raise ValueError('Invalid file_mode: provide a character octal like \"0644\" or \"644\".')\n\n    # Ensure there is a build log\n    logs = rxp_list_logs(project)\n    # rxp_list_logs raises if none; if it returned, we have log entries\n\n    # Read latest build log content via rxp_inspect (most recent)\n    rows = rxp_inspect(project_path=project, which_log=None)\n    if not isinstance(rows, list) or not rows:\n        raise RuntimeError(\"Could not read build log details; rxp_inspect returned no rows.\")\n\n    # Build a mapping from derivation name -&gt; list of store paths\n    # We try to be tolerant: look for keys 'derivation' (R), then 'deriv', 'name'\n    deriv_key_candidates = (\"derivation\", \"deriv\", \"name\")\n    path_key_candidates = (\"path\", \"store_path\", \"path_store\", \"output_path\", \"output\")\n\n    deriv_to_paths: Dict[str, List[str]] = {}\n    for r in rows:\n        if not isinstance(r, dict):\n            continue\n        deriv_val = _extract_field(r, deriv_key_candidates)\n        path_val = _extract_field(r, path_key_candidates)\n        if deriv_val is None:\n            # skip rows without a derivation name\n            continue\n        derivs = _ensure_iterable_of_strings(deriv_val)\n        paths = _ensure_iterable_of_strings(path_val)\n        for d in derivs:\n            deriv_to_paths.setdefault(d, []).extend(paths)\n\n    # Deduplicate path lists\n    for k in list(deriv_to_paths.keys()):\n        seen = []\n        for p in deriv_to_paths[k]:\n            if p not in seen:\n                seen.append(p)\n        deriv_to_paths[k] = seen\n\n    # Choose derivation_name if not provided\n    if derivation_name is None:\n        derivation_name = \"all-derivations\"\n\n    if derivation_name not in deriv_to_paths:\n        # Provide hint of available derivations (up to 20)\n        available = list(deriv_to_paths.keys())[:20]\n        more = \", ...\" if len(deriv_to_paths) &gt; 20 else \"\"\n        raise ValueError(\n            f\"No derivation {derivation_name!r} found in the build log. Available: {', '.join(available)}{more}\"\n        )\n\n    # Collect paths for this derivation\n    deriv_paths = deriv_to_paths.get(derivation_name, [])\n    if not deriv_paths:\n        raise RuntimeError(f\"No store paths recorded for derivation {derivation_name!r} in the build log.\")\n\n    output_dir = _ensure_output_dir(Path.cwd())\n\n    # For each store path, copy its contents into output_dir\n    copy_failed = False\n    errors: List[str] = []\n    for store_path_str in deriv_paths:\n        store_path = Path(store_path_str)\n        if not store_path.exists():\n            # Skip non-existing path (warn)\n            logger.warning(\"Store path does not exist, skipping: %s\", store_path)\n            continue\n        try:\n            # If the derivation path is a directory, copy its children into output_dir\n            if store_path.is_dir():\n                # copy each child into output_dir, preserving names\n                for child in store_path.iterdir():\n                    dest = output_dir / child.name\n                    if child.is_dir():\n                        # Python 3.8+: dirs_exist_ok True will merge\n                        try:\n                            shutil.copytree(child, dest, dirs_exist_ok=True)\n                        except TypeError:\n                            # older Python: fallback to manual merge\n                            if dest.exists():\n                                # copy contents into existing dest\n                                for sub in child.rglob(\"*\"):\n                                    rel = sub.relative_to(child)\n                                    target = dest / rel\n                                    if sub.is_dir():\n                                        target.mkdir(parents=True, exist_ok=True)\n                                    else:\n                                        target.parent.mkdir(parents=True, exist_ok=True)\n                                        shutil.copy2(sub, target)\n                            else:\n                                shutil.copytree(child, dest)\n                    else:\n                        # file: copy, possibly overwrite\n                        shutil.copy2(child, dest)\n            else:\n                # store_path is a file: copy into output_dir\n                dest_file = output_dir / store_path.name\n                shutil.copy2(store_path, dest_file)\n        except Exception as e:\n            copy_failed = True\n            errors.append(f\"{store_path}: {e}\")\n            logger.debug(\"Copy error for %s: %s\", store_path, e)\n\n    # Apply permissions\n    try:\n        _apply_permissions(output_dir, dir_mode=dir_mode, file_mode=file_mode)\n    except Exception:\n        # Best-effort: ignore permission application errors\n        logger.debug(\"Failed to apply permissions to %s\", output_dir)\n\n    if copy_failed:\n        raise RuntimeError(f\"Copy unsuccessful: errors occurred:\\n\" + \"\\n\".join(errors))\n\n    # Success message\n    print(f\"Copy successful, check out {output_dir}\")\n    return None\n</code></pre> <p>rxp_read</p> <p>Read the output of a derivation.</p> <p>Behavior:</p> <ul> <li>If resolved to multiple paths -&gt; return list[str].</li> <li>If single path:<ol> <li>If path is a directory -&gt; return the path string.</li> <li>Try to pickle.load the file (regardless of extension). If successful, return object.</li> <li>Try rds2py (if available) to parse; if successful, return object.</li> <li>Otherwise return the path string.</li> </ol> </li> </ul> <p>All failures are silent; no exceptions/warnings are raised for \"can't load\" cases.</p> Source code in <code>src/ryxpress/read_load.py</code> <pre><code>def rxp_read(\n    derivation_name: str,\n    which_log: Optional[str] = None,\n    project_path: Union[str, Path] = \".\",\n) -&gt; Union[object, str, List[str]]:\n    \"\"\"\n    rxp_read\n\n    Read the output of a derivation.\n\n    Behavior:\n\n    - If resolved to multiple paths -&gt; return list[str].\n    - If single path:\n        1. If path is a directory -&gt; return the path string.\n        2. Try to pickle.load the file (regardless of extension). If successful, return object.\n        3. Try rds2py (if available) to parse; if successful, return object.\n        4. Otherwise return the path string.\n\n    All failures are silent; no exceptions/warnings are raised for \"can't load\" cases.\n    \"\"\"\n    resolved = rxp_read_load_setup(derivation_name, which_log=which_log, project_path=project_path)\n\n    # If multiple outputs (list), return them directly\n    if isinstance(resolved, list):\n        return resolved\n\n    # Single path (string) or fallback value (derivation_name)\n    path = str(resolved)\n\n    # If path points to a directory, return it\n    if os.path.isdir(path):\n        return path\n\n    # Try to unpickle first (regardless of extension)\n    try:\n        with open(path, \"rb\") as fh:\n            obj = pickle.load(fh)\n        return obj\n    except Exception:\n        # Silent failure \u2014 try the next loader\n        logger.debug(\"pickle load failed for %s; will try rds2py if available\", path, exc_info=True)\n\n    # Try rds2py as a fallback (regardless of extension)\n    rds_obj = _load_rds_with_rds2py(path)\n    if rds_obj is not None:\n        return rds_obj\n\n    # Nothing worked; return the path string (no errors/warnings)\n    return path\n</code></pre> <p>rxp_load</p> <p>Load the output of a derivation into the caller's globals under the name <code>derivation_name</code> if successfully loaded as an object.</p> <p>Otherwise return the path(s) (string or list[str]). Silent on failures.</p> Source code in <code>src/ryxpress/read_load.py</code> <pre><code>def rxp_load(\n    derivation_name: str,\n    which_log: Optional[str] = None,\n    project_path: Union[str, Path] = \".\",\n) -&gt; Union[object, str, List[str]]:\n    \"\"\"\n    rxp_load\n\n    Load the output of a derivation into the caller's globals under the name\n    `derivation_name` if successfully loaded as an object.\n\n    Otherwise return the path(s) (string or list[str]). Silent on failures.\n    \"\"\"\n    resolved = rxp_read_load_setup(derivation_name, which_log=which_log, project_path=project_path)\n\n    # If multiple outputs, return them\n    if isinstance(resolved, list):\n        return resolved\n\n    path = str(resolved)\n\n    if os.path.isdir(path):\n        return path\n\n    # Try to unpickle first\n    try:\n        with open(path, \"rb\") as fh:\n            obj = pickle.load(fh)\n    except Exception:\n        obj = None\n        logger.debug(\"pickle load failed for %s; will try rds2py if available\", path, exc_info=True)\n\n    # If pickle failed, try rds2py\n    if obj is None:\n        obj = _load_rds_with_rds2py(path)\n\n    if obj is None:\n        # Nothing we can load silently; return the path\n        return path\n\n    # Assign into caller's globals (best-effort); silence any assignment errors\n    try:\n        caller_frame = inspect.currentframe().f_back\n        if caller_frame is not None:\n            caller_globals = caller_frame.f_globals\n            # Use derivation_name as the variable name; keep last path component if it's a path\n            try:\n                var_name = derivation_name\n                # If derivation_name looks like a path, use the basename without extension\n                if derivation_name.startswith(\"/nix/store/\") or os.path.sep in derivation_name:\n                    var_name = os.path.splitext(os.path.basename(str(path)))[0]\n                # ensure valid identifier fallback\n                if not var_name.isidentifier():\n                    var_name = \"_\".join(re.findall(r\"\\w+\", var_name)) or \"loaded_artifact\"\n            except Exception:\n                var_name = \"loaded_artifact\"\n            caller_globals[var_name] = obj\n    except Exception:\n        logger.debug(\"Failed to assign loaded object into caller globals\", exc_info=True)\n\n    return obj\n</code></pre>"},{"location":"reference/#visually-exploring-the-pipeline","title":"Visually exploring the pipeline","text":"<p>rxp_dag_for_ci</p> <p>Build an igraph object from nodes_and_edges and write a DOT file for CI.</p> <ul> <li>nodes_and_edges: dict with keys 'nodes' and 'edges' as returned by   get_nodes_edges(). If None, get_nodes_edges() is called.</li> <li>output_file: path to write DOT file. Parent directories are created as needed.</li> </ul> <p>Raises ImportError if python-igraph is not installed.</p> Source code in <code>src/ryxpress/plotting.py</code> <pre><code>def rxp_dag_for_ci(nodes_and_edges: Optional[Dict[str, List[Dict]]] = None,\n                   output_file: Union[str, Path] = \"_rixpress/dag.dot\") -&gt; None:\n    \"\"\"\n    rxp_dag_for_ci\n\n    Build an igraph object from nodes_and_edges and write a DOT file for CI.\n\n    - nodes_and_edges: dict with keys 'nodes' and 'edges' as returned by\n      get_nodes_edges(). If None, get_nodes_edges() is called.\n    - output_file: path to write DOT file. Parent directories are created as needed.\n\n    Raises ImportError if python-igraph is not installed.\n    \"\"\"\n    # Lazy import igraph and raise helpful error if not available\n    try:\n        import igraph  # python-igraph\n    except Exception as e:  # ImportError or other import-time errors\n        raise ImportError(\n            \"The python 'igraph' package is required for rxp_dag_for_ci. \"\n            \"Install it with e.g. 'pip install python-igraph' and try again.\"\n        ) from e\n\n    if nodes_and_edges is None:\n        nodes_and_edges = get_nodes_edges()\n\n    edges = nodes_and_edges.get(\"edges\", [])\n    # Build a list of tuples (from, to) for igraph\n    edge_tuples = [(e[\"from\"], e[\"to\"]) for e in edges]\n\n    # Ensure output directory exists\n    out_path = Path(output_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Create the graph from edge tuples. TupleList will create vertices named by\n    # the unique labels encountered in the tuples.\n    # If there are no edges but there are nodes, create an empty graph and add vertices.\n    if edge_tuples:\n        g = igraph.Graph.TupleList(edge_tuples, directed=True, vertex_name_attr=\"name\")\n    else:\n        # no edges \u2014 create graph and add vertices from nodes list\n        nodes = nodes_and_edges.get(\"nodes\", [])\n        vertex_names = [n[\"id\"] for n in nodes]\n        g = igraph.Graph(directed=True)\n        if vertex_names:\n            g.add_vertices(vertex_names)\n            # set the 'name' attribute automatically when vertices are named\n\n    # Set vertex 'label' attribute from vertex name\n    # g.vs['name'] should exist; copy to 'label'\n    try:\n        names = g.vs[\"name\"]\n        g.vs[\"label\"] = names\n        # Attempt to remove the 'name' attribute to mirror R behavior.\n        # python-igraph allows deleting vertex attributes via 'del g.vs[\"attr\"]'.\n        try:\n            del g.vs[\"name\"]\n        except Exception:\n            # If deletion is not supported in some igraph versions, leave it;\n            # having both 'name' and 'label' is harmless for DOT output.\n            logger.debug(\"Could not delete 'name' vertex attribute; leaving it in place.\")\n    except Exception:\n        # If the graph has no vertices or attribute access fails, continue.\n        pass\n\n    # Write graph to DOT format\n    # Use Graph.write with format=\"dot\"\n    try:\n        g.write(str(out_path), format=\"dot\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to write DOT file to {out_path}: {e}\") from e\n</code></pre> <p>Render a DOT graph file as an ASCII diagram using phart, showing node labels.</p> <p>This function reads a DOT file, parses it with pydot and networkx, and renders it in ASCII using phart. Node labels from the DOT file are used instead of numeric node IDs.</p> <p>rxp_trace</p> <p>Trace lineage of derivations.</p> <p>Returns:</p> <p>A dict mapping each inspected derivation name to a dict with keys:     - 'dependencies' : list of dependency names (ancestors), with transitive-only names marked with ''     - 'reverse_dependencies' : list of reverse dependents (children), with transitive-only names marked with ''</p> <p>Side-effect:</p> <p>Prints a tree representation to stdout (either the whole pipeline or   the single-node lineage).</p> Source code in <code>src/ryxpress/tracing.py</code> <pre><code>def rxp_trace(\n    name: Optional[str] = None,\n    dag_file: Union[str, Path] = Path(\"_rixpress\") / \"dag.json\",\n    transitive: bool = True,\n    include_self: bool = False,\n) -&gt; Dict[str, Dict[str, List[str]]]:\n    \"\"\"\n    rxp_trace\n\n    Trace lineage of derivations.\n\n    Returns:\n\n      A dict mapping each inspected derivation name to a dict with keys:\n        - 'dependencies' : list of dependency names (ancestors), with transitive-only names marked with '*'\n        - 'reverse_dependencies' : list of reverse dependents (children), with transitive-only names marked with '*'\n\n    Side-effect:\n\n      Prints a tree representation to stdout (either the whole pipeline or\n      the single-node lineage).\n    \"\"\"\n    derivs = _load_dag(dag_file)\n\n    all_names: List[str] = []\n    for d in derivs:\n        nm = _extract_name(d)\n        if nm is None:\n            raise ValueError(\"Found derivations with missing or unparsable names in dag.json.\")\n        all_names.append(nm)\n\n    if name is not None and name not in all_names:\n        # mirror R's head(...) behaviour for listing available names\n        snippet = \", \".join(all_names[:20])\n        more = \", ...\" if len(all_names) &gt; 20 else \"\"\n        raise ValueError(f\"Derivation '{name}' not found in dag.json (available: {snippet}{more}).\")\n\n    depends_map = _make_depends_map(derivs, all_names)\n    reverse_map = _build_reverse_map(depends_map, all_names)\n\n    # helper to print single lineage (deps and reverse deps)\n    def print_single(target: str) -&gt; None:\n        print(f\"==== Lineage for: {target} ====\")\n        # Dependencies (ancestors)\n        print(\"Dependencies (ancestors):\")\n        visited: List[str] = []\n\n        def rec_dep(node: str, depth: int) -&gt; None:\n            parents = depends_map.get(node) or []\n            if not parents:\n                if depth == 0:\n                    print(\"  - &lt;none&gt;\")\n                return\n            for p in parents:\n                label = f\"{p}*\" if (transitive and depth &gt;= 1) else p\n                print((\"  \" * (depth + 1)) + \"- \" + label)\n                if p not in visited:\n                    visited.append(p)\n                    rec_dep(p, depth + 1)\n\n        rec_dep(target, 0)\n\n        print(\"\\nReverse dependencies (children):\")\n        visited = []\n\n        def rec_rev(node: str, depth: int) -&gt; None:\n            kids = reverse_map.get(node) or []\n            if not kids:\n                if depth == 0:\n                    print(\"  - &lt;none&gt;\")\n                return\n            for k in kids:\n                label = f\"{k}*\" if (transitive and depth &gt;= 1) else k\n                print((\"  \" * (depth + 1)) + \"- \" + label)\n                if k not in visited:\n                    visited.append(k)\n                    rec_rev(k, depth + 1)\n\n        rec_rev(target, 0)\n\n        if transitive:\n            print(\"\\nNote: '*' marks transitive dependencies (depth &gt;= 2).\\n\")\n\n    # helper to print forest starting from given roots, using depends_map (outputs -&gt; inputs)\n    def print_forest_once(roots: List[str], graph: Dict[str, List[str]], transitive_flag: bool) -&gt; None:\n        visited_nodes: List[str] = []\n\n        def rec(node: str, depth: int) -&gt; None:\n            label = f\"{node}*\" if (transitive_flag and depth &gt;= 2) else node\n            print((\"  \" * depth) + \"- \" + label)\n            if node in visited_nodes:\n                return\n            visited_nodes.append(node)\n            kids = graph.get(node) or []\n            if not kids:\n                return\n            for k in kids:\n                rec(k, depth + 1)\n\n        for r in roots:\n            rec(r, 0)\n\n    # sinks: nodes with no children in reverse_map\n    def sinks() -&gt; List[str]:\n        no_children = [n for n, kids in reverse_map.items() if not kids]\n        if no_children:\n            return no_children\n        outdeg_vals = {n: len(kids) for n, kids in reverse_map.items()}\n        if outdeg_vals:\n            min_outdeg = min(outdeg_vals.values())\n            return [n for n, v in outdeg_vals.items() if v == min_outdeg]\n        return []\n\n    # Build results mapping\n    results: Dict[str, Dict[str, List[str]]] = {}\n    for nm in all_names:\n        deps = _marked_vec(nm, depends_map, transitive)\n        rdeps = _marked_vec(nm, reverse_map, transitive)\n        if include_self:\n            deps = _unique_preserve_order([nm] + deps)\n            rdeps = _unique_preserve_order([nm] + rdeps)\n        results[nm] = {\"dependencies\": deps, \"reverse_dependencies\": rdeps}\n\n    if name is None:\n        print(\"==== Pipeline dependency tree (outputs \\u2192 inputs) ====\")\n        for root in sinks():\n            print_forest_once([root], depends_map, transitive)\n        if transitive:\n            print(\"\\nNote: '*' marks transitive dependencies (depth &gt;= 2).\\n\")\n        return results\n    else:\n        print_single(name)\n        # return only the single-name mapping to match the R invisible(results[name]) behaviour\n        return {name: results[name]}\n</code></pre>"},{"location":"reference/#ryxpress.plotting.rxp_phart--dependencies","title":"Dependencies","text":"<ul> <li>phart</li> <li>pydot</li> <li>networkx</li> </ul> <p>If any dependency is missing, the function will print an instruction to install it.</p>"},{"location":"reference/#ryxpress.plotting.rxp_phart--parameters","title":"Parameters","text":"<p>dot_path : str     Path to the DOT file to render.</p>"},{"location":"reference/#ryxpress.plotting.rxp_phart--raises","title":"Raises","text":"<p>FileNotFoundError     If the specified DOT file does not exist. ValueError     If the DOT file is empty or cannot be parsed into a graph.</p> Source code in <code>src/ryxpress/plotting.py</code> <pre><code>def rxp_phart(dot_path: str) -&gt; None:\n    \"\"\"\n    Render a DOT graph file as an ASCII diagram using phart, showing node labels.\n\n    This function reads a DOT file, parses it with pydot and networkx, and\n    renders it in ASCII using phart. Node labels from the DOT file are used\n    instead of numeric node IDs.\n\n    Dependencies\n    ------------\n    - phart\n    - pydot\n    - networkx\n\n    If any dependency is missing, the function will print an instruction to install it.\n\n    Parameters\n    ----------\n    dot_path : str\n        Path to the DOT file to render.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified DOT file does not exist.\n    ValueError\n        If the DOT file is empty or cannot be parsed into a graph.\n    \"\"\"\n\n    # Dependency checks\n    missing = []\n    try:\n        import phart\n        from phart import ASCIIRenderer\n    except ImportError:\n        missing.append(\"phart\")\n    try:\n        import pydot\n    except ImportError:\n        missing.append(\"pydot\")\n    try:\n        import networkx as nx\n    except ImportError:\n        missing.append(\"networkx\")\n\n    if missing:\n        print(\n            f\"The following dependencies are required but not installed: {', '.join(missing)}\"\n        )\n        print(f\"Please add them to the execution environment.\")\n        return\n\n    # Check file exists\n    import os\n    if not os.path.exists(dot_path):\n        raise FileNotFoundError(f\"DOT file not found: {dot_path}\")\n\n    # Load DOT file\n    with open(dot_path) as f:\n        dot_data = f.read()\n\n    if not dot_data.strip():\n        raise ValueError(\"DOT file is empty.\")\n\n    # Parse DOT into networkx graph\n    graphs = pydot.graph_from_dot_data(dot_data)\n    if not graphs:\n        raise ValueError(\"No valid graphs found in DOT file.\")\n\n    G = nx.nx_pydot.from_pydot(graphs[0])\n\n    # Map node keys to labels for display\n    mapping = {node: data.get(\"label\", str(node)) for node, data in G.nodes(data=True)}\n    H = nx.relabel_nodes(G, mapping)\n\n    # Render ASCII\n    renderer = ASCIIRenderer(H)\n    print(renderer.render())\n</code></pre>"},{"location":"reference/#utilities","title":"Utilities","text":"<p>rxp_gc</p> <p>Garbage collect Nix store paths and build logs produced by rixpress.</p> <p>Parameters</p> <ul> <li>keep_since: None for full GC, or a date/ISO date string (YYYY-MM-DD) to keep logs newer-or-equal to that date.</li> <li>project_path: project root containing _rixpress</li> <li>dry_run: if True, show what would be deleted without deleting</li> <li>timeout_sec: timeout for invoked nix-store commands and for lock staleness checks</li> <li>verbose: if True, print extra diagnostic output</li> <li>ask: if True, prompt for confirmation before destructive operations (default True)</li> </ul> <p>Returns:</p> <p>A summary dict with canonical keys:     kept, deleted, protected, deleted_count, failed_count, referenced_count,     log_files_deleted, log_files_failed, dry_run_details</p> Source code in <code>src/ryxpress/garbage.py</code> <pre><code>def rxp_gc(\n    keep_since: Optional[Union[str, date]] = None,\n    project_path: Union[str, Path] = \".\",\n    dry_run: bool = True,\n    timeout_sec: int = 300,\n    verbose: bool = False,\n    ask: bool = True,\n    pretty: bool = False,\n    as_json: bool = False,\n) -&gt; Dict[str, object]:\n    \"\"\"\n    rxp_gc\n\n    Garbage collect Nix store paths and build logs produced by rixpress.\n\n    Parameters\n\n    - keep_since: None for full GC, or a date/ISO date string (YYYY-MM-DD) to keep logs newer-or-equal to that date.\n    - project_path: project root containing _rixpress\n    - dry_run: if True, show what would be deleted without deleting\n    - timeout_sec: timeout for invoked nix-store commands and for lock staleness checks\n    - verbose: if True, print extra diagnostic output\n    - ask: if True, prompt for confirmation before destructive operations (default True)\n\n    Returns:\n\n      A summary dict with canonical keys:\n        kept, deleted, protected, deleted_count, failed_count, referenced_count,\n        log_files_deleted, log_files_failed, dry_run_details\n    \"\"\"\n    nix_bin = shutil.which(\"nix-store\")\n    if not nix_bin:\n        raise FileNotFoundError(\"nix-store not found on PATH. Install Nix or adjust PATH.\")\n\n    project_path = Path(project_path).resolve()\n    if not project_path.exists():\n        raise FileNotFoundError(f\"Project path does not exist: {project_path}\")\n\n    lock_file_path = Path(tempfile.gettempdir()) / \"rixpress_gc.lock\"\n\n    # record of temp gcroot symlink paths we created so we can remove them later\n    created_gcroot_links: List[Path] = []\n\n    # ensure we cleanup on signals\n    def _cleanup_on_signal(signum, frame):\n        logger.info(\"Received signal %s, cleaning up...\", signum)\n        # remove any gcroot links\n        for p in created_gcroot_links:\n            try:\n                if p.exists():\n                    p.unlink()\n            except Exception:\n                pass\n        # remove lock file if held\n        try:\n            if lock_path_context and lock_path_context.acquired:\n                lock_path_context.release()\n        except Exception:\n            pass\n        raise SystemExit(1)\n\n    # placeholder for context so signal handler can access\n    lock_path_context: Optional[LockFile] = None\n\n    # Register handlers\n    old_sigint = signal.getsignal(signal.SIGINT)\n    old_sigterm = signal.getsignal(signal.SIGTERM)\n    signal.signal(signal.SIGINT, _cleanup_on_signal)\n    signal.signal(signal.SIGTERM, _cleanup_on_signal)\n\n    try:\n        # Acquire lock with context manager (atomic)\n        lock_path_context = LockFile(lock_file_path, timeout_sec=timeout_sec)\n        lock_path_context.acquire()\n\n        # parse keep_since\n        if keep_since is not None:\n            if isinstance(keep_since, date) and not isinstance(keep_since, datetime):\n                keep_date = keep_since\n            else:\n                # accept YYYY-MM-DD string\n                try:\n                    keep_date = _parse_iso_date(str(keep_since))\n                except Exception:\n                    raise ValueError(\"Invalid 'keep_since'. Use a date or 'YYYY-MM-DD' string.\")\n        else:\n            keep_date = None\n\n        # Gather logs\n        all_logs = rxp_list_logs(project_path)\n        # Expect list of dicts with 'filename' and 'modification_time'\n        if not isinstance(all_logs, list) or not all_logs:\n            logger.info(\"No build logs found. Nothing to do.\")\n            # canonical empty summary\n            return {\n                \"kept\": [],\n                \"deleted\": [],\n                \"protected\": 0,\n                \"deleted_count\": 0,\n                \"failed_count\": 0,\n                \"referenced_count\": 0,\n                \"log_files_deleted\": 0,\n                \"log_files_failed\": 0,\n                \"dry_run_details\": None,\n            }\n\n        # Partition logs\n        logs_to_keep = []\n        logs_to_delete = []\n        for entry in all_logs:\n            fn = entry.get(\"filename\")\n            mtime = entry.get(\"modification_time\")\n            if not fn or not mtime:\n                continue\n            try:\n                mdate = _parse_iso_date(mtime)\n            except Exception:\n                # If malformed, treat as older than keep_since to be conservative\n                mdate = datetime.min.date()\n            if keep_date is None:\n                logs_to_keep.append(entry)\n            else:\n                if mdate &gt;= keep_date:\n                    logs_to_keep.append(entry)\n                else:\n                    logs_to_delete.append(entry)\n\n        def _filenames(entries: Sequence[Dict]) -&gt; List[str]:\n            return [e[\"filename\"] for e in entries]\n\n        # helper to get store paths per log using rxp_inspect\n        def get_paths_from_logs(filenames: Sequence[str]) -&gt; Dict[str, List[str]]:\n            out: Dict[str, List[str]] = {}\n            for fn in filenames:\n                wl = _extract_which_log(fn)\n                if wl is None:\n                    logger.warning(\"Could not parse which_log from filename: %s\", fn)\n                    out[fn] = []\n                    continue\n                try:\n                    insp_rows = rxp_inspect(project_path=project_path, which_log=wl)\n                except Exception as e:\n                    logger.warning(\"rxp_inspect failed for %s: %s\", fn, e)\n                    out[fn] = []\n                    continue\n                # rxp_inspect returns list of dicts; look for 'path' keys\n                paths = []\n                if isinstance(insp_rows, list):\n                    for row in insp_rows:\n                        if isinstance(row, dict) and \"path\" in row and isinstance(row[\"path\"], str):\n                            paths.append(row[\"path\"])\n                out[fn] = _validate_store_paths(paths)\n            return out\n\n        keep_paths_by_log = get_paths_from_logs(_filenames(logs_to_keep)) if logs_to_keep else {}\n        delete_paths_by_log = get_paths_from_logs(_filenames(logs_to_delete)) if logs_to_delete else {}\n\n        keep_paths_all = _validate_store_paths(sorted({p for lst in keep_paths_by_log.values() for p in lst}))\n        delete_paths_all = _validate_store_paths(sorted({p for lst in delete_paths_by_log.values() for p in lst}))\n\n        summary_info: Dict[str, object] = {\n            \"kept\": _filenames(logs_to_keep),\n            \"deleted\": _filenames(logs_to_delete),\n            \"protected\": 0,\n            \"deleted_count\": 0,\n            \"failed_count\": 0,\n            \"referenced_count\": 0,\n            \"log_files_deleted\": 0,\n            \"log_files_failed\": 0,\n            \"dry_run_details\": None,\n        }\n\n        # DRY RUN branch (date-based)\n        if keep_date is not None and dry_run:\n            logger.info(\"--- DRY RUN --- No changes will be made. ---\")\n            logger.info(\"Logs that would be deleted (%d):\", len(logs_to_delete))\n            for fn in summary_info[\"deleted\"]:\n                logger.info(\"  %s\", fn)\n            details: Dict[str, List[Dict[str, str]]] = {}\n            if delete_paths_by_log:\n                logger.info(\"Artifacts per log (from rxp_inspect):\")\n                for fn, _ in delete_paths_by_log.items():\n                    logger.info(\"== %s ==\", fn)\n                    try:\n                        insp_rows = rxp_inspect(project_path=project_path, which_log=_extract_which_log(fn) or \"\")\n                    except Exception:\n                        logger.info(\"  (rxp_inspect unavailable)\")\n                        details[fn] = []\n                        continue\n                    rows = []\n                    if isinstance(insp_rows, list):\n                        for r in insp_rows:\n                            if not isinstance(r, dict):\n                                continue\n                            rows.append({\"path\": r.get(\"path\", \"\"), \"output\": r.get(\"output\", \"\")})\n                    details[fn] = rows\n            existing_delete_paths = [p for p in delete_paths_all if os.path.exists(p) or os.path.isdir(p)]\n            missing_paths = [p for p in delete_paths_all if p not in existing_delete_paths]\n            logger.info(\"Aggregate store paths targeted for deletion (deduped): %d total, %d existing, %d missing\",\n                        len(delete_paths_all), len(existing_delete_paths), len(missing_paths))\n            if existing_delete_paths:\n                logger.info(\"Existing paths that would be deleted:\")\n                for p in existing_delete_paths:\n                    logger.info(\"  %s\", p)\n            if missing_paths:\n                logger.info(\"Paths already missing (will be skipped):\")\n                for p in missing_paths:\n                    logger.info(\"  %s\", p)\n            summary_info[\"dry_run_details\"] = details\n            if logs_to_delete:\n                logger.info(\"Build log files that would be deleted:\")\n                for fn in summary_info[\"deleted\"]:\n                    log_path = project_path / \"_rixpress\" / fn\n                    exists_indicator = \"[OK]\" if log_path.exists() else \"[X]\"\n                    logger.info(\"  %s %s\", exists_indicator, fn)\n            if pretty:\n                if as_json:\n                    print(json.dumps(summary_info, indent=2, ensure_ascii=False))\n                else:\n                    pprint(summary_info)\n                return\n\n            return summary_info\n\n        # dry-run full GC preview\n        if keep_date is None and dry_run:\n            logger.info(\"--- DRY RUN --- Would run 'nix-store --gc' (delete all unreferenced store paths). ---\")\n            if verbose:\n                logger.info(\"(Tip: for an approximate preview, run 'nix-collect-garbage -n' from a shell.)\")\n            return summary_info\n\n        # Full GC mode\n        if keep_date is None:\n            if ask:\n                proceed = _ask_yes_no(\"Run full Nix garbage collection (delete all unreferenced artifacts)?\", default=False)\n                if not proceed:\n                    logger.info(\"Operation cancelled.\")\n                    return summary_info\n            logger.info(\"Running Nix garbage collector...\")\n            try:\n                _, stdout, stderr = _safe_run([nix_bin, \"--gc\"], timeout=timeout_sec, check=True)\n                if stdout:\n                    if verbose:\n                        logger.info(stdout)\n                    else:\n                        rel = [l for l in stdout.splitlines() if re.search(r\"freed|removing|deleting\", l, re.I)]\n                        if rel:\n                            for line in rel[-10:]:\n                                logger.info(line)\n                logger.info(\"Garbage collection complete.\")\n                return summary_info\n            except RxpGCError as e:\n                raise\n\n        # Targeted deletion mode\n        if not logs_to_delete:\n            logger.info(\"No build logs older than %s found. Nothing to do.\", keep_date.isoformat())\n            return summary_info\n\n        if not delete_paths_all:\n            logger.info(\"No valid store paths found in logs older than %s. Nothing to delete.\", keep_date.isoformat())\n            return summary_info\n\n        prompt = f\"This will permanently delete {len(delete_paths_all)} store paths from {len(logs_to_delete)} build(s) older than {keep_date.isoformat()}. Continue?\"\n        if ask:\n            if not _ask_yes_no(prompt, default=False):\n                logger.info(\"Operation cancelled.\")\n                return summary_info\n\n        # Protect recent artifacts (date-based mode only) by adding indirect GC roots.\n        temp_gcroots_dir: Optional[Path] = None\n        protected = 0\n        try:\n            if keep_paths_all:\n                temp_gcroots_dir = Path(tempfile.mkdtemp(prefix=\"rixpress-gc-\"))\n                logger.info(\"Protecting %d recent artifacts via GC roots...\", len(keep_paths_all))\n                for i, p in enumerate(keep_paths_all, start=1):\n                    link_path = temp_gcroots_dir / f\"root-{i}\"\n                    try:\n                        # create a placeholder link path (the nix-store --add-root will create the gcroot)\n                        # use link_path as the path to register the indirect root\n                        _safe_run([nix_bin, \"--add-root\", str(link_path), \"--indirect\", p], timeout=timeout_sec, check=True)\n                        created_gcroot_links.append(link_path)\n                        protected += 1\n                    except RxpGCError as e:\n                        logger.warning(\"Failed to add GC root for %s: %s\", p, e)\n                if protected == 0:\n                    raise RxpGCError(\"Failed to protect any store paths. Aborting.\")\n                summary_info[\"protected\"] = protected\n\n            # Delete specific store paths\n            logger.info(\"Deleting %d targeted store paths...\", len(delete_paths_all))\n            existing_paths = [p for p in delete_paths_all if os.path.exists(p) or os.path.isdir(p)]\n            missing_paths = [p for p in delete_paths_all if p not in existing_paths]\n            if missing_paths:\n                logger.info(\"Skipping %d paths that no longer exist.\", len(missing_paths))\n                if verbose:\n                    for p in missing_paths:\n                        logger.info(\"  Missing: %s\", p)\n            if not existing_paths:\n                logger.info(\"No existing paths to delete. All targeted paths are already gone.\")\n                return summary_info\n\n            total_deleted = 0\n            failed_paths: List[str] = []\n            referenced_paths: List[str] = []\n\n            for i, pth in enumerate(existing_paths, start=1):\n                if not (os.path.exists(pth) or os.path.isdir(pth)):\n                    logger.info(\"  [%d/%d] Skipping %s (already gone)\", i, len(existing_paths), os.path.basename(pth))\n                    continue\n                logger.info(\"  [%d/%d] Attempting to delete %s...\", i, len(existing_paths), os.path.basename(pth))\n                try:\n                    proc = subprocess.run([nix_bin, \"--delete\", pth], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=timeout_sec)\n                    out = (proc.stdout or \"\") + \"\\n\" + (proc.stderr or \"\")\n                    if proc.returncode == 0:\n                        total_deleted += 1\n                        logger.info(\"    [OK] Successfully deleted\")\n                        if verbose and out.strip():\n                            logger.info(\"    %s\", out.strip())\n                    else:\n                        if re.search(r\"still alive|Cannot delete\", out, re.I):\n                            referenced_paths.append(pth)\n                            logger.info(\"    [!] Skipped (still referenced)\")\n                            if verbose:\n                                logger.info(\"    Details: %s\", out.strip())\n                        else:\n                            failed_paths.append(pth)\n                            logger.info(\"    [X] Failed to delete\")\n                            if verbose:\n                                logger.info(\"    Details: %s\", out.strip())\n                except subprocess.TimeoutExpired:\n                    failed_paths.append(pth)\n                    logger.info(\"    [X] Timeout while deleting\")\n                except Exception as e:\n                    failed_paths.append(pth)\n                    logger.info(\"    [X] Error: %s\", e)\n\n            # Summary of deletion\n            logger.info(\"\\nDeletion summary:\")\n            logger.info(\"  Successfully deleted: %d paths\", total_deleted)\n            logger.info(\"  Skipped (still referenced): %d paths\", len(referenced_paths))\n            logger.info(\"  Failed (other errors): %d paths\", len(failed_paths))\n\n            if referenced_paths and verbose:\n                logger.info(\"\\nReferenced paths (cannot delete):\")\n                for pth in referenced_paths:\n                    logger.info(\"  %s\", os.path.basename(pth))\n                    try:\n                        _, roots_out, _ = _safe_run([nix_bin, \"--query\", \"--roots\", pth], timeout=timeout_sec, check=False)\n                        if roots_out.strip():\n                            logger.info(\"    GC roots: %s\", roots_out.strip().replace(\"\\n\", \", \"))\n                        else:\n                            logger.info(\"    GC roots: (none found)\")\n                    except Exception:\n                        logger.info(\"    GC roots: (query failed)\")\n                    try:\n                        _, refs_out, _ = _safe_run([nix_bin, \"--query\", \"--referrers\", pth], timeout=timeout_sec, check=False)\n                        if refs_out.strip():\n                            refs = [os.path.basename(x) for x in refs_out.splitlines() if x.strip()]\n                            logger.info(\"    Referenced by: %s\", \", \".join(refs) if refs else \"(none)\")\n                        else:\n                            logger.info(\"    Referenced by: (none)\")\n                    except Exception:\n                        logger.info(\"    Referenced by: (query failed)\")\n\n            summary_info[\"deleted_count\"] = total_deleted\n            summary_info[\"failed_count\"] = len(failed_paths)\n            summary_info[\"referenced_count\"] = len(referenced_paths)\n\n            # Delete old build log files\n            if logs_to_delete:\n                logger.info(\"\\nDeleting old build log files...\")\n                log_files_deleted = 0\n                log_files_failed: List[str] = []\n                for i, entry in enumerate(logs_to_delete, start=1):\n                    log_file = entry[\"filename\"]\n                    log_path = project_path / \"_rixpress\" / log_file\n                    logger.info(\"  [%d/%d] Deleting %s...\", i, len(logs_to_delete), log_file)\n                    if not log_path.exists():\n                        logger.info(\"    [!] File not found (already deleted?)\")\n                        continue\n                    try:\n                        log_path.unlink()\n                        if not log_path.exists():\n                            log_files_deleted += 1\n                            logger.info(\"    [OK] Successfully deleted\")\n                        else:\n                            log_files_failed.append(log_file)\n                            logger.info(\"    [X] Failed to delete (file still exists)\")\n                    except Exception as e:\n                        log_files_failed.append(log_file)\n                        logger.info(\"    [X] Error: %s\", e)\n                logger.info(\"\\nBuild log deletion summary:\")\n                logger.info(\"  Successfully deleted: %d files\", log_files_deleted)\n                logger.info(\"  Failed: %d files\", len(log_files_failed))\n                if log_files_failed and verbose:\n                    logger.info(\"\\nFailed to delete log files:\")\n                    for lf in log_files_failed:\n                        logger.info(\"  %s\", lf)\n                summary_info[\"log_files_deleted\"] = log_files_deleted\n                summary_info[\"log_files_failed\"] = len(log_files_failed)\n\n            logger.info(\"\\nCleanup complete!\")\n            return summary_info\n        finally:\n            # Always attempt to remove created gcroot links and the temp dir\n            if created_gcroot_links:\n                for p in created_gcroot_links:\n                    try:\n                        if p.exists():\n                            p.unlink()\n                    except Exception:\n                        logger.debug(\"Failed to unlink gcroot link %s\", p)\n                # attempt to remove the parent temp directory if exists and empty\n                if temp_gcroots_dir and temp_gcroots_dir.exists():\n                    try:\n                        shutil.rmtree(temp_gcroots_dir)\n                    except Exception:\n                        # ignore: best-effort cleanup\n                        logger.debug(\"Failed to remove temp gcroots dir %s\", temp_gcroots_dir)\n    finally:\n        # always release lock and restore signals\n        try:\n            if lock_path_context is not None:\n                lock_path_context.release()\n        except Exception:\n            pass\n        try:\n            signal.signal(signal.SIGINT, old_sigint)\n            signal.signal(signal.SIGTERM, old_sigterm)\n        except Exception:\n            pass\n</code></pre>"}]}